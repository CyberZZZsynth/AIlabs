# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО       | Роль в проекте                     | Оценка       |
|-----------|------------------------------------|--------------|
| Степанов Д. | Реализовал часть 1 и 3 |       |
| Тихонов Ф.| Реализовал часть 4 |      |
| Зайцев К. | Реализовал часть 2 |          |

# Начало

Мы работаем с датасетом Movie Subtitles.

Сперва перед обучением с нейросетью поработаем с датасетом. В первую очередь необходимо удалить все незаполненные ячейки с текстом, все значения перевели в строки.

## SimpleRNN

RNN --- это рекурентная нейронная сеть. В них есть циклы, которые помогают для обработки последовательностей. Если приводить текст к числовой последовательности, то RNN позволяет её обработать и спрогнозировать новый вывод последовательности. Такая архитектура идеально подходит для задач генерации текста

Мы сделали токенизатор, который преобразовывает строки в числа. Дальше реализовали архитектуру сети, состоящую из трех слоев: Embedding, SimpleRNN и Dense. В Embedding происходит преобразование индексов для каждого слова в вектор. Дальше происходит соответственно обработка этих векторов, а слой Dense прогнозирует новое слово.

Дальше мы обучили её и попробовали сгенерировать текст.

Затем для нашей модели мы сделали посимвольную генерацию. Также обучили её и попробовали сгенерировать текст

## Unidirectional single-layer LSTM words

Рекуррентные нейронные сети добавляют память к искуственным нейронным сетям, но реализуемая память получается короткой — на каждом шаге обучения информация в памяти смешивается с новой и через несколько итераций полностью перезаписывается. LSTM-модули разработаны специально, чтобы избежать проблемы долговременной зависимости, запоминая значения как на короткие, так и на длинные промежутки времени.

В этом пункте мы реализовали такую LSTM сеть и обучили её. Также сгенерировали текст

Позже также сделали аналогичную модель с посимвольной генерацией.

## Bidirectional LSTM

Все ранее рассмотренные архитектуры рекуррентных нейронных сетей были однонаправленными: они обрабатывали входной сигнал последовательно во времени. Но иногда лучше проводить обработку и в прямом и в обратном направлениях одновременно. Например, мы хотим спрогнозировать недостающее слово по его окружающему контексту. Для этого нужно знать и прошлый и будущий контекст. Как раз для такого рода задач и была предложена двунаправленная LSTM. Её архитектура достаточно проста и представлена двумя рекуррентными слоями, разворачивающихся в противоположных направлениях.

Мы создали модель, обучили её и сгенерировали текст

## Вывод по пунткам 1-3

Токенизация посимольно показала себя хуже, чем токенизация по словам, так как при генерации по буквам мы не смотрим на смысл контекста и смысл слов, а смотрим только на распределение вероятности встречи буквы после буквы, из-за этого получаются последовательности, в которых сочетания букв похоже на кусочки слов, но к сожалению, в таких последовательностях смысла мало.

Лучший текст, сгенерированный нейросетью, получился такой:
Jenny gonna today hey who none of the hey sarge have can be happy ha bad news

## GPT архитектура с нуля

Следующим этапом была работа с GPT-архитектурой.

Её архитектура сложнее, чем у рекурентных нейросетей. Список токенов проходит через Embedding Layer (линейный слой) и превращается в список эмбеддингов. Дальше к каждому эмбеддингу прибавляется positional embedding. Он позволяет видеть порядок входных токенов для генерации осмысленного текста. После этого идет Transformer Decoder Block. После того как список эмбеддингов пройдёт через последний блок, эмбеддинг, соответствующий последнему токену матрично умножается на всё тот же входной, но уже транспонированный Embedding Layer и после применения SoftMax получается распределение вероятностей следующего токена. 

Из-за более сложной архитектуры, текст получается более осмысленный. 

b"[BOS] hello friend , and welcome home this day i ' m owners are having a new surrence [PAD]l . . . [PAD] . . [PAD] to a britrating , that ' s going along a fox ? [PAD] [PAD] 8"

Проблемой является то, что нейросеть генерирует много паддингов. Устранить эту проблему у нас к сожалению, не вышло. 





