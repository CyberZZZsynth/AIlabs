{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jaAUvSnu8Sq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdLi6wHuBsn1",
        "outputId": "f42c66cc-855e-4072-987a-574cd4bcaced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/movies_subtitles.csv\", engine=\"python\")\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "t7HJ44bevbgh",
        "outputId": "729deb29-ace8-448f-9a61-7aef48890807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   start_time  end_time                                               text  \\\n",
              "0      58.559    61.602  BOY: All right, everyone!\\nThis... is a stick-up!   \n",
              "1      61.687    63.354                                Don't anybody move!   \n",
              "2      64.398    66.482                              Now, empty that safe!   \n",
              "3      68.318    71.612       Ooh-hoo-hoo!\\nMoney, money, money! (KISSING)   \n",
              "4      71.697    74.031           Stop it! Stop it,\\nyou mean, old potato!   \n",
              "\n",
              "     imdb_id  \n",
              "0  tt0114709  \n",
              "1  tt0114709  \n",
              "2  tt0114709  \n",
              "3  tt0114709  \n",
              "4  tt0114709  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c6eee2e-d89f-4c38-b5a9-b01ab86e6113\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>text</th>\n",
              "      <th>imdb_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58.559</td>\n",
              "      <td>61.602</td>\n",
              "      <td>BOY: All right, everyone!\\nThis... is a stick-up!</td>\n",
              "      <td>tt0114709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61.687</td>\n",
              "      <td>63.354</td>\n",
              "      <td>Don't anybody move!</td>\n",
              "      <td>tt0114709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.398</td>\n",
              "      <td>66.482</td>\n",
              "      <td>Now, empty that safe!</td>\n",
              "      <td>tt0114709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>68.318</td>\n",
              "      <td>71.612</td>\n",
              "      <td>Ooh-hoo-hoo!\\nMoney, money, money! (KISSING)</td>\n",
              "      <td>tt0114709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71.697</td>\n",
              "      <td>74.031</td>\n",
              "      <td>Stop it! Stop it,\\nyou mean, old potato!</td>\n",
              "      <td>tt0114709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c6eee2e-d89f-4c38-b5a9-b01ab86e6113')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c6eee2e-d89f-4c38-b5a9-b01ab86e6113 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c6eee2e-d89f-4c38-b5a9-b01ab86e6113');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "После попытки обучения модели на датасете и появлении ошибки я догадался, что с данными что-то не так. Оказалось, что в данных целых 27 строк с незаполненным текстом. Выведу первые пять таких строк."
      ],
      "metadata": {
        "id": "q1qJmvU38v13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[data[\"text\"].isna()].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "695FXOAT8wIS",
        "outputId": "10879d03-6e4f-45fb-d76f-96468231ed5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        start_time  end_time text    imdb_id\n",
              "7452        6898.0  6901.134  NaN  tt0112637\n",
              "32877       5495.0  5498.084  NaN  tt0112495\n",
              "41861       5087.0  5090.095  NaN  tt0112508\n",
              "83269       7205.0  7208.084  NaN  tt0111742\n",
              "102811      5603.0  5606.136  NaN  tt0107756"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faf46be3-5d75-4702-a14e-26e166c40780\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>text</th>\n",
              "      <th>imdb_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7452</th>\n",
              "      <td>6898.0</td>\n",
              "      <td>6901.134</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tt0112637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32877</th>\n",
              "      <td>5495.0</td>\n",
              "      <td>5498.084</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tt0112495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41861</th>\n",
              "      <td>5087.0</td>\n",
              "      <td>5090.095</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tt0112508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83269</th>\n",
              "      <td>7205.0</td>\n",
              "      <td>7208.084</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tt0111742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102811</th>\n",
              "      <td>5603.0</td>\n",
              "      <td>5606.136</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tt0107756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faf46be3-5d75-4702-a14e-26e166c40780')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-faf46be3-5d75-4702-a14e-26e166c40780 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-faf46be3-5d75-4702-a14e-26e166c40780');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вообще удалим такие строки, так как мы не знаем, чем их лучше всего заполнить"
      ],
      "metadata": {
        "id": "nOE7Cbn0hwob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(data[data[\"text\"].isna()].index)\n",
        "data[data['text'].isna()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "t_e0oqfL84zN",
        "outputId": "90377944-c21d-492f-8c36-176f04dda9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [start_time, end_time, text, imdb_id]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30abd5d6-d696-4274-b49c-d60766996e5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>text</th>\n",
              "      <th>imdb_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30abd5d6-d696-4274-b49c-d60766996e5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30abd5d6-d696-4274-b49c-d60766996e5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30abd5d6-d696-4274-b49c-d60766996e5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(x):\n",
        "    return x[\"text\"]"
      ],
      "metadata": {
        "id": "4ZJoC7QpwONH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = np.array(get_text(data.loc[:1000000]))\n",
        "texts = np.array(texts)\n",
        "(texts)\n",
        "max_tokens = 5000\n",
        "texts"
      ],
      "metadata": {
        "id": "fMAOvwXMzbEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64547422-3b75-4649-80d5-64289e830213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['BOY: All right, everyone! This... is a stick-up!',\n",
              "       \"Don't anybody move!\", 'Now, empty that safe!', ...,\n",
              "       \"If she stumbled on a burglary, she should've been here.\",\n",
              "       'But she was killed exactly where she is.',\n",
              "       'See, the blood patterns indicate that.'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(x):\n",
        "   x = str(x)\n",
        "   x = x.replace('\\n', ' ')\n",
        "   x = x.encode(\"utf8\").decode(\"ascii\", 'ignore')\n",
        "   return x"
      ],
      "metadata": {
        "id": "L8dPgkkysEIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"word_count\"] = data[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "data[\"text\"] = data[\"text\"].apply(transform)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "1dGFX-Njt_WN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0cd57fe7-c552-4860-dea6-51a580c0ded6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   start_time  end_time                                              text  \\\n",
              "0      58.559    61.602  BOY: All right, everyone! This... is a stick-up!   \n",
              "1      61.687    63.354                               Don't anybody move!   \n",
              "2      64.398    66.482                             Now, empty that safe!   \n",
              "3      68.318    71.612       Ooh-hoo-hoo! Money, money, money! (KISSING)   \n",
              "4      71.697    74.031           Stop it! Stop it, you mean, old potato!   \n",
              "\n",
              "     imdb_id  word_count  \n",
              "0  tt0114709           8  \n",
              "1  tt0114709           3  \n",
              "2  tt0114709           4  \n",
              "3  tt0114709           5  \n",
              "4  tt0114709           8  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1499e1a6-e539-4f27-829a-f4081af0f1a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>text</th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58.559</td>\n",
              "      <td>61.602</td>\n",
              "      <td>BOY: All right, everyone! This... is a stick-up!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61.687</td>\n",
              "      <td>63.354</td>\n",
              "      <td>Don't anybody move!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.398</td>\n",
              "      <td>66.482</td>\n",
              "      <td>Now, empty that safe!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>68.318</td>\n",
              "      <td>71.612</td>\n",
              "      <td>Ooh-hoo-hoo! Money, money, money! (KISSING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71.697</td>\n",
              "      <td>74.031</td>\n",
              "      <td>Stop it! Stop it, you mean, old potato!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1499e1a6-e539-4f27-829a-f4081af0f1a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1499e1a6-e539-4f27-829a-f4081af0f1a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1499e1a6-e539-4f27-829a-f4081af0f1a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = data[\"word_count\"].max()\n",
        "max_len"
      ],
      "metadata": {
        "id": "3nxQUd3pub4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838eeb3a-08d0-44b7-e689-1fd42ce008fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3123"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 5000\n",
        "vocab_size = 5000\n",
        "batch_size = 128\n",
        "embed_size = 64\n",
        "samples_size = 1000000"
      ],
      "metadata": {
        "id": "65Flbf01GO74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_tokens, input_shape=(1,))\n",
        "vectorizer.adapt(texts)"
      ],
      "metadata": {
        "id": "uKyWapapwSJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(vectorizer.get_vocabulary())"
      ],
      "metadata": {
        "id": "j_4GUsraFLk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe69a45c-6ad7-4488-f868-c7c7260fddc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'you',\n",
              " 'the',\n",
              " 'i',\n",
              " 'to',\n",
              " 'a',\n",
              " 'and',\n",
              " 'it',\n",
              " 'of',\n",
              " 'that',\n",
              " 'in',\n",
              " 'me',\n",
              " 'is',\n",
              " 'what',\n",
              " 'this',\n",
              " 'on',\n",
              " 'for',\n",
              " 'my',\n",
              " 'your',\n",
              " 'do',\n",
              " 'no',\n",
              " 'dont',\n",
              " 'we',\n",
              " 'have',\n",
              " 'im',\n",
              " 'be',\n",
              " 'know',\n",
              " 'he',\n",
              " 'all',\n",
              " 'its',\n",
              " 'not',\n",
              " 'are',\n",
              " 'was',\n",
              " 'with',\n",
              " 'get',\n",
              " 'here',\n",
              " 'just',\n",
              " 'right',\n",
              " 'like',\n",
              " 'but',\n",
              " 'go',\n",
              " 'come',\n",
              " 'so',\n",
              " 'out',\n",
              " 'up',\n",
              " 'him',\n",
              " 'youre',\n",
              " 'got',\n",
              " 'well',\n",
              " 'now',\n",
              " 'about',\n",
              " 'can',\n",
              " 'there',\n",
              " 'if',\n",
              " 'at',\n",
              " 'oh',\n",
              " 'thats',\n",
              " 'they',\n",
              " 'were',\n",
              " 'one',\n",
              " 'yeah',\n",
              " 'see',\n",
              " 'want',\n",
              " 'how',\n",
              " 'good',\n",
              " 'think',\n",
              " 'her',\n",
              " 'will',\n",
              " 'did',\n",
              " 'his',\n",
              " 'she',\n",
              " 'as',\n",
              " 'look',\n",
              " 'yes',\n",
              " 'ill',\n",
              " 'why',\n",
              " 'from',\n",
              " 'hes',\n",
              " 'gonna',\n",
              " 'going',\n",
              " 'man',\n",
              " 'take',\n",
              " 'back',\n",
              " 'when',\n",
              " 'time',\n",
              " 'okay',\n",
              " 'them',\n",
              " 'who',\n",
              " 'an',\n",
              " 'cant',\n",
              " 'l',\n",
              " 'tell',\n",
              " 'us',\n",
              " 'say',\n",
              " 'some',\n",
              " 'been',\n",
              " 'down',\n",
              " 'or',\n",
              " 'hey',\n",
              " 'would',\n",
              " 'had',\n",
              " 'where',\n",
              " 'never',\n",
              " 'could',\n",
              " 'little',\n",
              " 'way',\n",
              " 'very',\n",
              " 'didnt',\n",
              " 'then',\n",
              " 'whats',\n",
              " 'too',\n",
              " 'over',\n",
              " 'mr',\n",
              " 'something',\n",
              " 'by',\n",
              " 'more',\n",
              " 'make',\n",
              " 'our',\n",
              " 'let',\n",
              " 'give',\n",
              " 'really',\n",
              " 'mean',\n",
              " 'ive',\n",
              " 'off',\n",
              " 'lets',\n",
              " 'love',\n",
              " 'theres',\n",
              " 'please',\n",
              " 'has',\n",
              " 'any',\n",
              " 'said',\n",
              " 'thank',\n",
              " 'need',\n",
              " 'sure',\n",
              " 'sir',\n",
              " 'two',\n",
              " 'sorry',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'am',\n",
              " 'people',\n",
              " 'much',\n",
              " 'because',\n",
              " 'doing',\n",
              " 'these',\n",
              " 'help',\n",
              " 'god',\n",
              " 'uh',\n",
              " 'should',\n",
              " 'anything',\n",
              " 'wait',\n",
              " 'better',\n",
              " '♪',\n",
              " 'call',\n",
              " 'shes',\n",
              " 'put',\n",
              " 'day',\n",
              " 'talk',\n",
              " 'theyre',\n",
              " 'first',\n",
              " 'nothing',\n",
              " 'maybe',\n",
              " 'again',\n",
              " 'night',\n",
              " 'great',\n",
              " 'even',\n",
              " 'big',\n",
              " 'into',\n",
              " 'ever',\n",
              " 'must',\n",
              " 'find',\n",
              " 'home',\n",
              " 'life',\n",
              " 'away',\n",
              " 'those',\n",
              " 'thought',\n",
              " 'around',\n",
              " 'stop',\n",
              " 'money',\n",
              " 'before',\n",
              " 'hell',\n",
              " 'than',\n",
              " 'new',\n",
              " 'still',\n",
              " 'id',\n",
              " 'work',\n",
              " 'old',\n",
              " 'keep',\n",
              " 'guy',\n",
              " 'wont',\n",
              " 'does',\n",
              " 'other',\n",
              " 'things',\n",
              " 'youve',\n",
              " 'long',\n",
              " 'always',\n",
              " 'last',\n",
              " 'shit',\n",
              " 'guys',\n",
              " 'their',\n",
              " 'youll',\n",
              " 'years',\n",
              " 'after',\n",
              " 'nice',\n",
              " 'told',\n",
              " 'believe',\n",
              " 'leave',\n",
              " 'boy',\n",
              " 'hear',\n",
              " 'name',\n",
              " 'gotta',\n",
              " 'hello',\n",
              " 'feel',\n",
              " 'wanna',\n",
              " 'listen',\n",
              " 'stay',\n",
              " 'father',\n",
              " 'miss',\n",
              " 'three',\n",
              " 'everything',\n",
              " 'isnt',\n",
              " 'fuck',\n",
              " 'coming',\n",
              " 'remember',\n",
              " 'hi',\n",
              " 'kind',\n",
              " 'place',\n",
              " 'every',\n",
              " 'thanks',\n",
              " 'aint',\n",
              " 'understand',\n",
              " 'bad',\n",
              " 'huh',\n",
              " 'through',\n",
              " 'lot',\n",
              " 'made',\n",
              " 'girl',\n",
              " 'may',\n",
              " 'fine',\n",
              " 'doesnt',\n",
              " 'wrong',\n",
              " 'another',\n",
              " 'house',\n",
              " 'left',\n",
              " 'hold',\n",
              " 'show',\n",
              " 'kill',\n",
              " 'enough',\n",
              " 'done',\n",
              " 'mind',\n",
              " 'son',\n",
              " 'dead',\n",
              " 'ask',\n",
              " 'talking',\n",
              " 'course',\n",
              " 'car',\n",
              " 'might',\n",
              " 'own',\n",
              " 'move',\n",
              " 'fucking',\n",
              " 'care',\n",
              " 'baby',\n",
              " 'morning',\n",
              " 'happened',\n",
              " 'real',\n",
              " 'wanted',\n",
              " 'try',\n",
              " 'friend',\n",
              " 'world',\n",
              " 'mother',\n",
              " 'being',\n",
              " 'ii',\n",
              " 'yourself',\n",
              " 'play',\n",
              " 'wouldnt',\n",
              " 'best',\n",
              " 'next',\n",
              " 'dad',\n",
              " 'came',\n",
              " 'seen',\n",
              " 'else',\n",
              " 'which',\n",
              " 'minute',\n",
              " 'woman',\n",
              " 'ok',\n",
              " 'run',\n",
              " 'excuse',\n",
              " 'getting',\n",
              " 'wasnt',\n",
              " 'matter',\n",
              " 'watch',\n",
              " 'room',\n",
              " 'guess',\n",
              " 'today',\n",
              " 'whole',\n",
              " 'job',\n",
              " 'men',\n",
              " 'head',\n",
              " 'same',\n",
              " 'heard',\n",
              " 'most',\n",
              " 'tonight',\n",
              " 'trying',\n",
              " 'weve',\n",
              " 'live',\n",
              " 'pretty',\n",
              " 'everybody',\n",
              " 'kid',\n",
              " 'idea',\n",
              " 'many',\n",
              " 'looking',\n",
              " 'saw',\n",
              " 'says',\n",
              " 'went',\n",
              " 'knew',\n",
              " 'once',\n",
              " 'ready',\n",
              " 'turn',\n",
              " 'without',\n",
              " 'together',\n",
              " 'sit',\n",
              " 'school',\n",
              " 'shut',\n",
              " 'youd',\n",
              " 'happy',\n",
              " 'found',\n",
              " 'myself',\n",
              " 'havent',\n",
              " 'someone',\n",
              " 'bring',\n",
              " 'open',\n",
              " 'beautiful',\n",
              " 'meet',\n",
              " 'wife',\n",
              " 'tomorrow',\n",
              " 'somebody',\n",
              " 'em',\n",
              " 'used',\n",
              " 'use',\n",
              " 'alone',\n",
              " 'couldnt',\n",
              " 'business',\n",
              " 'friends',\n",
              " 'hope',\n",
              " 'honey',\n",
              " 'lm',\n",
              " 'gone',\n",
              " 'iand',\n",
              " 'yet',\n",
              " 'mrs',\n",
              " 'family',\n",
              " 'problem',\n",
              " 'damn',\n",
              " 'took',\n",
              " 'days',\n",
              " 'whos',\n",
              " 'since',\n",
              " 'start',\n",
              " 'boys',\n",
              " 'young',\n",
              " 'such',\n",
              " 'killed',\n",
              " 'hand',\n",
              " 'easy',\n",
              " 'five',\n",
              " 'door',\n",
              " 'while',\n",
              " 'forget',\n",
              " 'kids',\n",
              " 'die',\n",
              " 'face',\n",
              " 'wants',\n",
              " 'cause',\n",
              " 'called',\n",
              " 'mom',\n",
              " 'game',\n",
              " 'saying',\n",
              " 'ah',\n",
              " 'wheres',\n",
              " 'eat',\n",
              " 'few',\n",
              " 'both',\n",
              " 'looks',\n",
              " 'hard',\n",
              " 'already',\n",
              " 'police',\n",
              " 'gun',\n",
              " 'hit',\n",
              " 'late',\n",
              " 'second',\n",
              " 'phone',\n",
              " 'lost',\n",
              " 'crazy',\n",
              " 'each',\n",
              " 'worry',\n",
              " 'stuff',\n",
              " 'minutes',\n",
              " 'ass',\n",
              " 'nobody',\n",
              " 'read',\n",
              " 'year',\n",
              " 'pay',\n",
              " 'brother',\n",
              " 'true',\n",
              " 'later',\n",
              " 'end',\n",
              " 'drink',\n",
              " 'hurt',\n",
              " 'part',\n",
              " 'mine',\n",
              " 'lady',\n",
              " 'bit',\n",
              " 'ya',\n",
              " 'shot',\n",
              " 'four',\n",
              " 'knows',\n",
              " 'stand',\n",
              " 'case',\n",
              " 'eyes',\n",
              " 'until',\n",
              " 'jesus',\n",
              " 'under',\n",
              " 'ago',\n",
              " 'comes',\n",
              " 'um',\n",
              " 'i♪',\n",
              " '♪i',\n",
              " 'wish',\n",
              " 'cut',\n",
              " 'afraid',\n",
              " 'far',\n",
              " 'town',\n",
              " 'story',\n",
              " 'daddy',\n",
              " 'dear',\n",
              " 'change',\n",
              " 'shall',\n",
              " 'goes',\n",
              " 'water',\n",
              " 'exactly',\n",
              " 'hands',\n",
              " 'fire',\n",
              " 'probably',\n",
              " 'times',\n",
              " 'thinking',\n",
              " 'point',\n",
              " 'gave',\n",
              " 'goddamn',\n",
              " 'makes',\n",
              " 'anybody',\n",
              " 'trouble',\n",
              " 'quite',\n",
              " 'week',\n",
              " 'actually',\n",
              " 'sleep',\n",
              " 'anyway',\n",
              " 'number',\n",
              " 'shoot',\n",
              " 'close',\n",
              " 'set',\n",
              " 'word',\n",
              " 'mama',\n",
              " 'waiting',\n",
              " 'whatever',\n",
              " 'funny',\n",
              " 'happen',\n",
              " 'anyone',\n",
              " 'heart',\n",
              " 'soon',\n",
              " 'married',\n",
              " 'ahead',\n",
              " 'ha',\n",
              " 'walk',\n",
              " 'check',\n",
              " 'taking',\n",
              " 'gets',\n",
              " 'chance',\n",
              " 'arent',\n",
              " 'supposed',\n",
              " 'doctor',\n",
              " 'death',\n",
              " 'deal',\n",
              " 'john',\n",
              " 'having',\n",
              " 'speak',\n",
              " 'party',\n",
              " 'bye',\n",
              " 'half',\n",
              " 'question',\n",
              " 'bitch',\n",
              " 'gentlemen',\n",
              " 'hate',\n",
              " 'heres',\n",
              " 'girls',\n",
              " 'break',\n",
              " 'important',\n",
              " 'children',\n",
              " 'against',\n",
              " 'whoa',\n",
              " 'fun',\n",
              " 'rest',\n",
              " 'inside',\n",
              " 'bed',\n",
              " 'six',\n",
              " 'working',\n",
              " 'ithe',\n",
              " 'yours',\n",
              " 'person',\n",
              " 'war',\n",
              " 'country',\n",
              " 'truth',\n",
              " 'hours',\n",
              " 'jack',\n",
              " 'office',\n",
              " 'lord',\n",
              " 'hurry',\n",
              " 'making',\n",
              " 'means',\n",
              " 'white',\n",
              " 'also',\n",
              " 'send',\n",
              " 'different',\n",
              " 'bet',\n",
              " 'along',\n",
              " 'met',\n",
              " 'answer',\n",
              " 'pick',\n",
              " 'dog',\n",
              " 'couple',\n",
              " 'fight',\n",
              " 'george',\n",
              " 'line',\n",
              " 'side',\n",
              " 'goodbye',\n",
              " 'everyone',\n",
              " 'dr',\n",
              " 'telling',\n",
              " 'welcome',\n",
              " 'behind',\n",
              " 'sick',\n",
              " 'least',\n",
              " 'lts',\n",
              " 'front',\n",
              " 'theyll',\n",
              " 'stupid',\n",
              " 'book',\n",
              " 'anymore',\n",
              " 'high',\n",
              " 'full',\n",
              " 'almost',\n",
              " 'child',\n",
              " 'free',\n",
              " 'black',\n",
              " 'started',\n",
              " 'asked',\n",
              " 'sometimes',\n",
              " 'poor',\n",
              " 'moment',\n",
              " 'buy',\n",
              " 'tried',\n",
              " 'bill',\n",
              " 'though',\n",
              " 'dinner',\n",
              " 'city',\n",
              " 'sort',\n",
              " 'light',\n",
              " 'body',\n",
              " 'either',\n",
              " 'lose',\n",
              " 'king',\n",
              " 'reason',\n",
              " 'street',\n",
              " 'itll',\n",
              " 'husband',\n",
              " 'fuckin',\n",
              " 'million',\n",
              " 'iyou',\n",
              " 'pull',\n",
              " 'music',\n",
              " 'outside',\n",
              " 'seems',\n",
              " 'between',\n",
              " 'drive',\n",
              " 'playing',\n",
              " 'fact',\n",
              " 'women',\n",
              " 'captain',\n",
              " 'york',\n",
              " 'general',\n",
              " '1',\n",
              " 'sister',\n",
              " 'glad',\n",
              " 'ten',\n",
              " 'till',\n",
              " 'touch',\n",
              " 'news',\n",
              " 'hang',\n",
              " 'clear',\n",
              " 'ones',\n",
              " 'trust',\n",
              " 'quiet',\n",
              " 'tom',\n",
              " 'shouldnt',\n",
              " 'luck',\n",
              " 'frank',\n",
              " 'max',\n",
              " 'alive',\n",
              " 'write',\n",
              " 'dream',\n",
              " 'scared',\n",
              " 'wonderful',\n",
              " 'hot',\n",
              " 'hour',\n",
              " 'cool',\n",
              " 'died',\n",
              " 'red',\n",
              " 'company',\n",
              " 'president',\n",
              " 'feeling',\n",
              " 'promise',\n",
              " 'brought',\n",
              " '…',\n",
              " 'christmas',\n",
              " 'save',\n",
              " 'beat',\n",
              " 'lives',\n",
              " '20',\n",
              " 'picture',\n",
              " 'sweet',\n",
              " 'months',\n",
              " 'blood',\n",
              " 'hmm',\n",
              " 'air',\n",
              " 'coffee',\n",
              " 'buddy',\n",
              " 'himself',\n",
              " 'evening',\n",
              " 'hows',\n",
              " 'running',\n",
              " 'hair',\n",
              " 'state',\n",
              " 'ball',\n",
              " 'special',\n",
              " 'taken',\n",
              " 'able',\n",
              " 'dance',\n",
              " 'safe',\n",
              " 'ladies',\n",
              " 'catch',\n",
              " 'careful',\n",
              " 'feet',\n",
              " 'maam',\n",
              " 'perhaps',\n",
              " 'seem',\n",
              " 'suppose',\n",
              " 'order',\n",
              " 'needs',\n",
              " 'piece',\n",
              " 'lucky',\n",
              " 'daughter',\n",
              " 'road',\n",
              " 'fall',\n",
              " 'power',\n",
              " 'serious',\n",
              " 'billy',\n",
              " 'youi',\n",
              " 'follow',\n",
              " 'win',\n",
              " 'early',\n",
              " 'cold',\n",
              " 'top',\n",
              " 'drop',\n",
              " 'team',\n",
              " 'sound',\n",
              " 'seven',\n",
              " 'throw',\n",
              " 'questions',\n",
              " 'perfect',\n",
              " 'known',\n",
              " 'joe',\n",
              " 'learn',\n",
              " 'clean',\n",
              " 'straight',\n",
              " 'goin',\n",
              " 'kiss',\n",
              " 'wed',\n",
              " 'lll',\n",
              " 'hed',\n",
              " 'words',\n",
              " 'ride',\n",
              " 'living',\n",
              " 'step',\n",
              " 'miles',\n",
              " 'fast',\n",
              " 'darling',\n",
              " 'cannot',\n",
              " 'sign',\n",
              " 'wonder',\n",
              " 'david',\n",
              " 'sent',\n",
              " 'somewhere',\n",
              " 'looked',\n",
              " 'tired',\n",
              " '—',\n",
              " 'outta',\n",
              " 'mike',\n",
              " 'marry',\n",
              " 'honor',\n",
              " 'become',\n",
              " 'doin',\n",
              " 'asking',\n",
              " 'small',\n",
              " 'la',\n",
              " 'leaving',\n",
              " 'human',\n",
              " 'past',\n",
              " 'others',\n",
              " 'lie',\n",
              " 'christ',\n",
              " 'mad',\n",
              " 'boat',\n",
              " 'sounds',\n",
              " 'wake',\n",
              " 'mei',\n",
              " 'plan',\n",
              " 'absolutely',\n",
              " 'happens',\n",
              " 'land',\n",
              " 'american',\n",
              " 'michael',\n",
              " 'worth',\n",
              " '10',\n",
              " 'movie',\n",
              " 'lt',\n",
              " 'i\\x92m',\n",
              " 'ibut',\n",
              " 'wear',\n",
              " 'weeks',\n",
              " 'calling',\n",
              " 'van',\n",
              " 'tv',\n",
              " 'rather',\n",
              " 'born',\n",
              " 'takes',\n",
              " 'finally',\n",
              " 'felt',\n",
              " 'class',\n",
              " 'wow',\n",
              " 'blow',\n",
              " 'laughing',\n",
              " 'theyve',\n",
              " 'food',\n",
              " 'coach',\n",
              " 'quick',\n",
              " 'ooh',\n",
              " 'law',\n",
              " 'blue',\n",
              " 'earth',\n",
              " 'mouth',\n",
              " 'act',\n",
              " 'america',\n",
              " 'sense',\n",
              " 'key',\n",
              " 'sex',\n",
              " 'pass',\n",
              " 'control',\n",
              " 'none',\n",
              " 'certainly',\n",
              " 'moving',\n",
              " 'figure',\n",
              " 'clothes',\n",
              " 'handle',\n",
              " 'loved',\n",
              " 'charlie',\n",
              " 'present',\n",
              " 'longer',\n",
              " 'army',\n",
              " '2',\n",
              " 'simple',\n",
              " 'de',\n",
              " 'bullshit',\n",
              " 'werent',\n",
              " 'strange',\n",
              " 'rock',\n",
              " 'less',\n",
              " 'floor',\n",
              " 'fool',\n",
              " 'murder',\n",
              " 'quit',\n",
              " 'attention',\n",
              " 'possible',\n",
              " 'birthday',\n",
              " 'except',\n",
              " 'worked',\n",
              " 'across',\n",
              " 'sam',\n",
              " 'shell',\n",
              " 'smart',\n",
              " 'ray',\n",
              " 'expect',\n",
              " 'harry',\n",
              " 'lovely',\n",
              " 'happening',\n",
              " 'eye',\n",
              " 'bob',\n",
              " 'watching',\n",
              " 'court',\n",
              " 'kidding',\n",
              " 'hungry',\n",
              " 'fair',\n",
              " 'lieutenant',\n",
              " 'broke',\n",
              " 'given',\n",
              " 'fly',\n",
              " 'art',\n",
              " 'anywhere',\n",
              " 'parents',\n",
              " 'ought',\n",
              " 'meeting',\n",
              " 'dollars',\n",
              " 'dark',\n",
              " 'tough',\n",
              " 'ring',\n",
              " 'rose',\n",
              " 'pleasure',\n",
              " 'caught',\n",
              " 'letter',\n",
              " 'giving',\n",
              " 'pain',\n",
              " 'charge',\n",
              " 'unless',\n",
              " 'works',\n",
              " 'strong',\n",
              " 'forgive',\n",
              " 'bar',\n",
              " 'jimmy',\n",
              " 'calm',\n",
              " 'professor',\n",
              " 'report',\n",
              " 'forgot',\n",
              " 'uncle',\n",
              " 'stick',\n",
              " 'hotel',\n",
              " 'paid',\n",
              " 'cop',\n",
              " 'relax',\n",
              " 'certain',\n",
              " 'busy',\n",
              " 'paper',\n",
              " 'bought',\n",
              " 'boss',\n",
              " 'fault',\n",
              " 'finish',\n",
              " 'chief',\n",
              " 'eight',\n",
              " 'voice',\n",
              " 'hospital',\n",
              " 'secret',\n",
              " 'mommy',\n",
              " 'worse',\n",
              " 'mistake',\n",
              " 'college',\n",
              " 'plane',\n",
              " 'record',\n",
              " 'james',\n",
              " 'rich',\n",
              " 'sell',\n",
              " 'iits',\n",
              " 'explain',\n",
              " 'truck',\n",
              " 'iti',\n",
              " 'forever',\n",
              " 'dress',\n",
              " 'card',\n",
              " 'swear',\n",
              " 'liked',\n",
              " 'month',\n",
              " 'middle',\n",
              " 'shh',\n",
              " 'radio',\n",
              " 'terrible',\n",
              " 'kept',\n",
              " 'cops',\n",
              " 'box',\n",
              " 'date',\n",
              " 'cover',\n",
              " 'afternoon',\n",
              " 'sitting',\n",
              " 'ran',\n",
              " 'peace',\n",
              " 'future',\n",
              " 'station',\n",
              " 'train',\n",
              " 'proud',\n",
              " 'changed',\n",
              " 'teach',\n",
              " 'gold',\n",
              " 'talked',\n",
              " 'jim',\n",
              " 'imagine',\n",
              " 'won',\n",
              " 'turned',\n",
              " 'thinks',\n",
              " 'meant',\n",
              " 'jump',\n",
              " 'senator',\n",
              " 'sergeant',\n",
              " 'table',\n",
              " 'dick',\n",
              " 'nothin',\n",
              " 'lunch',\n",
              " 'calls',\n",
              " 'lf',\n",
              " 'doc',\n",
              " 'difference',\n",
              " 'fell',\n",
              " 'colonel',\n",
              " 'yo',\n",
              " 'cry',\n",
              " 'decided',\n",
              " 'bag',\n",
              " 'sing',\n",
              " 'third',\n",
              " 'mmm',\n",
              " 'interested',\n",
              " 'horse',\n",
              " 'finished',\n",
              " 'near',\n",
              " 'missed',\n",
              " 'ship',\n",
              " 'count',\n",
              " 'surprise',\n",
              " 'fathers',\n",
              " 'fat',\n",
              " 'choice',\n",
              " 'message',\n",
              " 'idiot',\n",
              " 'fix',\n",
              " 'eh',\n",
              " 'window',\n",
              " 'personal',\n",
              " 'standing',\n",
              " 'position',\n",
              " 'fish',\n",
              " 'round',\n",
              " 'return',\n",
              " 'cat',\n",
              " 'seeing',\n",
              " 'heaven',\n",
              " 'club',\n",
              " 'age',\n",
              " 'names',\n",
              " 'bank',\n",
              " 'short',\n",
              " 'ia',\n",
              " 'asshole',\n",
              " 'somethin',\n",
              " 'major',\n",
              " 'bastard',\n",
              " 'building',\n",
              " 'spend',\n",
              " 'private',\n",
              " 'worried',\n",
              " 'guns',\n",
              " 'grace',\n",
              " 'folks',\n",
              " 'eddie',\n",
              " 'hat',\n",
              " '3',\n",
              " 'song',\n",
              " 'thousand',\n",
              " 'soul',\n",
              " 'smell',\n",
              " 'slow',\n",
              " 'missing',\n",
              " 'mess',\n",
              " 'lve',\n",
              " 'join',\n",
              " 'ground',\n",
              " 'offer',\n",
              " 'church',\n",
              " 'lawyer',\n",
              " 'entire',\n",
              " 'needed',\n",
              " 'english',\n",
              " 'trip',\n",
              " 'ice',\n",
              " 'jail',\n",
              " 'holy',\n",
              " 'hasnt',\n",
              " 'realize',\n",
              " 'neither',\n",
              " 'grab',\n",
              " 'detective',\n",
              " 'apartment',\n",
              " '50',\n",
              " 'bucks',\n",
              " 'push',\n",
              " 'cash',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(None, 5000)),\n",
        "    tf.keras.layers.Masking(mask_value=0),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "model.build((None, None, 5000))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_EkFtlsHGKY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81b0069-2e28-4095-8e1a-7f0c0c6d59d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking (Masking)           (None, None, 5000)        0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         2626048   \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 5000)        645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,271,048\n",
            "Trainable params: 3,271,048\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(19, embed_size),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GNUgHEYyhmt",
        "outputId": "519b611f-bb5a-466a-e950-601c6a2d7f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, None, 64)          1216      \n",
            "                                                                 \n",
            " lstm_25 (LSTM)              (None, 128)               98816     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 5000)              645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 745,032\n",
            "Trainable params: 745,032\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(x):\n",
        "  x = vectorizer(x)\n",
        "  y = x[:, 1:]\n",
        "  col = tf.constant(2, shape=(x.shape[0],1), dtype=\"int64\")\n",
        "  y = tf.concat([y, col], 1)\n",
        "  out1 = tf.one_hot(x, vocab_size)\n",
        "  out2 = tf.one_hot(y, vocab_size)\n",
        "  return out1, out2"
      ],
      "metadata": {
        "id": "nbxUTtQfvqhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(x):\n",
        "  x = vectorizer(x)\n",
        "  y = x[:, 1:]\n",
        "  col = tf.constant(0, shape=(x.shape[0],1), dtype=\"int64\")\n",
        "  y = tf.concat([y, col], 1)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "6XFGuxVezs6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare([\"hello world\", \"nice to meet you\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruPQq5Bw-itm",
        "outputId": "a66699d6-21b9-436d-a601-7f4ac6ef4a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
              " array([[212, 269,   0,   0],\n",
              "        [204,   5, 333,   2]])>,\n",
              " <tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
              " array([[269,   0,   0,   0],\n",
              "        [  5, 333,   2,   0]])>)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Протестируем функцию"
      ],
      "metadata": {
        "id": "WpJ1uYhNtmXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"hello world\", \"nice to meet you\"]\n",
        "prepare_data(x)"
      ],
      "metadata": {
        "id": "ChwzLwe0tm6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450a14e2-a2d1-459a-db67-406d9930e698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 4, 5000), dtype=float32, numpy=\n",
              " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2, 4, 5000), dtype=float32, numpy=\n",
              " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0006), loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "index=0\n",
        "X, Y = prepare_data(np.array(get_text(data.loc[index * batch_size:index * batch_size + batch_size])))\n",
        "model.fit(X, Y,epochs=200)\n"
      ],
      "metadata": {
        "id": "2pT9MTla-TvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5542c4-3ad7-4855-d383-074b7fd7ab79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 7s 250ms/step - loss: 0.0785 - acc: 0.9664\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0778 - acc: 0.9680\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 1s 226ms/step - loss: 0.0769 - acc: 0.9685\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0762 - acc: 0.9690\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 1s 268ms/step - loss: 0.0754 - acc: 0.9690\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 2s 404ms/step - loss: 0.0755 - acc: 0.9695\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 2s 408ms/step - loss: 0.0756 - acc: 0.9695\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.0754 - acc: 0.9695\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0754 - acc: 0.9685\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0753 - acc: 0.9685\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0751 - acc: 0.9690\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.0749 - acc: 0.9690\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0749 - acc: 0.9690\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0748 - acc: 0.9695\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 0.0747 - acc: 0.9685\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 2s 389ms/step - loss: 0.0746 - acc: 0.9685\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 2s 393ms/step - loss: 0.0746 - acc: 0.9674\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 2s 303ms/step - loss: 0.0745 - acc: 0.9685\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 1s 231ms/step - loss: 0.0745 - acc: 0.9685\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.0744 - acc: 0.9690\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.0744 - acc: 0.9680\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.0744 - acc: 0.9680\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0744 - acc: 0.9685\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0743 - acc: 0.9674\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0744 - acc: 0.9674\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 2s 313ms/step - loss: 0.0743 - acc: 0.9674\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 2s 404ms/step - loss: 0.0743 - acc: 0.9685\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 2s 387ms/step - loss: 0.0743 - acc: 0.9695\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0742 - acc: 0.9690\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.0742 - acc: 0.9695\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0742 - acc: 0.9695\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0741 - acc: 0.9685\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.0741 - acc: 0.9685\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0740 - acc: 0.9695\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.0739 - acc: 0.9695\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0739 - acc: 0.9690\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 2s 404ms/step - loss: 0.0738 - acc: 0.9674\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 2s 406ms/step - loss: 0.0738 - acc: 0.9685\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 0.0738 - acc: 0.9680\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0738 - acc: 0.9685\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.0739 - acc: 0.9695\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.0739 - acc: 0.9695\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0739 - acc: 0.9680\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0738 - acc: 0.9690\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 1s 233ms/step - loss: 0.0738 - acc: 0.9690\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0739 - acc: 0.9685\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 2s 394ms/step - loss: 0.0739 - acc: 0.9690\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 2s 396ms/step - loss: 0.0739 - acc: 0.9690\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 2s 289ms/step - loss: 0.0738 - acc: 0.9685\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0737 - acc: 0.9674\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0736 - acc: 0.9685\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0739 - acc: 0.9695\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0740 - acc: 0.9690\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 0.0740 - acc: 0.9695\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0741 - acc: 0.9690\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.0740 - acc: 0.9690\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 2s 319ms/step - loss: 0.0739 - acc: 0.9690\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 0.0738 - acc: 0.9695\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0740 - acc: 0.9690\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0740 - acc: 0.9680\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 1s 231ms/step - loss: 0.0741 - acc: 0.9680\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0740 - acc: 0.9690\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0740 - acc: 0.9685\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 1s 233ms/step - loss: 0.0740 - acc: 0.9680\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.0739 - acc: 0.9680\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0738 - acc: 0.9690\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 1s 229ms/step - loss: 0.0736 - acc: 0.9690\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 2s 394ms/step - loss: 0.0735 - acc: 0.9685\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 2s 397ms/step - loss: 0.0734 - acc: 0.9695\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.0734 - acc: 0.9695\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 1s 236ms/step - loss: 0.0732 - acc: 0.9690\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0731 - acc: 0.9690\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 1s 229ms/step - loss: 0.0731 - acc: 0.9695\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.0730 - acc: 0.9685\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0733 - acc: 0.9680\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0736 - acc: 0.9685\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0735 - acc: 0.9685\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 2s 319ms/step - loss: 0.0734 - acc: 0.9685\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 2s 385ms/step - loss: 0.0733 - acc: 0.9690\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 2s 385ms/step - loss: 0.0731 - acc: 0.9690\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 1s 227ms/step - loss: 0.0733 - acc: 0.9690\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 1s 232ms/step - loss: 0.0732 - acc: 0.9680\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.0733 - acc: 0.9685\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 1s 232ms/step - loss: 0.0733 - acc: 0.9685\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.0732 - acc: 0.9685\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0731 - acc: 0.9695\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0730 - acc: 0.9680\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 1s 236ms/step - loss: 0.0728 - acc: 0.9685\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 2s 403ms/step - loss: 0.0733 - acc: 0.9685\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 2s 408ms/step - loss: 0.0733 - acc: 0.9690\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 2s 269ms/step - loss: 0.0733 - acc: 0.9690\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0732 - acc: 0.9685\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 1s 228ms/step - loss: 0.0731 - acc: 0.9685\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0735 - acc: 0.9680\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0735 - acc: 0.9685\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.0732 - acc: 0.9685\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0736 - acc: 0.9690\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0732 - acc: 0.9695\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 2s 362ms/step - loss: 0.0730 - acc: 0.9690\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 2s 393ms/step - loss: 0.0731 - acc: 0.9695\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 2s 332ms/step - loss: 0.0731 - acc: 0.9695\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 0.0733 - acc: 0.9690\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 1s 232ms/step - loss: 0.0737 - acc: 0.9690\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 1s 253ms/step - loss: 0.0732 - acc: 0.9690\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.0734 - acc: 0.9690\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0731 - acc: 0.9690\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0730 - acc: 0.9695\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0728 - acc: 0.9695\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 2s 312ms/step - loss: 0.0729 - acc: 0.9690\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 2s 405ms/step - loss: 0.0728 - acc: 0.9695\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 0.0726 - acc: 0.9685\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 1s 264ms/step - loss: 0.0725 - acc: 0.9690\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 1s 249ms/step - loss: 0.0723 - acc: 0.9695\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 1s 252ms/step - loss: 0.0723 - acc: 0.9690\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0721 - acc: 0.9695\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 1s 270ms/step - loss: 0.0719 - acc: 0.9690\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 2s 327ms/step - loss: 0.0718 - acc: 0.9695\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 2s 404ms/step - loss: 0.0718 - acc: 0.9690\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 2s 384ms/step - loss: 0.0719 - acc: 0.9685\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0719 - acc: 0.9680\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.0719 - acc: 0.9690\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0719 - acc: 0.9695\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 1s 254ms/step - loss: 0.0721 - acc: 0.9685\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 1s 255ms/step - loss: 0.0723 - acc: 0.9685\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0726 - acc: 0.9685\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 1s 249ms/step - loss: 0.0726 - acc: 0.9690\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 1s 275ms/step - loss: 0.0729 - acc: 0.9690\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 2s 400ms/step - loss: 0.0728 - acc: 0.9690\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 2s 416ms/step - loss: 0.0727 - acc: 0.9690\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 0.0725 - acc: 0.9685\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0724 - acc: 0.9690\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.0723 - acc: 0.9690\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 1s 252ms/step - loss: 0.0721 - acc: 0.9690\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0719 - acc: 0.9695\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0719 - acc: 0.9685\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 2s 400ms/step - loss: 0.0719 - acc: 0.9680\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 2s 398ms/step - loss: 0.0718 - acc: 0.9680\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0718 - acc: 0.9690\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 1s 254ms/step - loss: 0.0718 - acc: 0.9690\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0718 - acc: 0.9690\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0723 - acc: 0.9690\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.0732 - acc: 0.9685\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 0.0731 - acc: 0.9685\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0725 - acc: 0.9690\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0768 - acc: 0.9669\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 2s 389ms/step - loss: 0.0743 - acc: 0.9685\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 2s 397ms/step - loss: 0.0750 - acc: 0.9680\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 2s 322ms/step - loss: 0.0838 - acc: 0.9643\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.1793 - acc: 0.9499\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.4331 - acc: 0.9075\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.1874 - acc: 0.9328\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 1s 230ms/step - loss: 0.1796 - acc: 0.9421\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.1181 - acc: 0.9540\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 1s 236ms/step - loss: 0.1394 - acc: 0.9483\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 1s 228ms/step - loss: 0.1022 - acc: 0.9561\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 1s 252ms/step - loss: 0.0795 - acc: 0.9664\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 2s 409ms/step - loss: 0.0882 - acc: 0.9633\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 2s 402ms/step - loss: 0.0770 - acc: 0.9680\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0787 - acc: 0.9674\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0748 - acc: 0.9690\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0758 - acc: 0.9674\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 1s 229ms/step - loss: 0.0744 - acc: 0.9690\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 1s 233ms/step - loss: 0.0738 - acc: 0.9690\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.0743 - acc: 0.9685\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0740 - acc: 0.9680\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 0.0736 - acc: 0.9690\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 2s 383ms/step - loss: 0.0734 - acc: 0.9685\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 2s 396ms/step - loss: 0.0733 - acc: 0.9690\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 2s 324ms/step - loss: 0.0732 - acc: 0.9690\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.0730 - acc: 0.9685\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 1s 232ms/step - loss: 0.0730 - acc: 0.9690\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.0728 - acc: 0.9695\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.0727 - acc: 0.9695\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 0.0726 - acc: 0.9695\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 1s 249ms/step - loss: 0.0725 - acc: 0.9695\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.0726 - acc: 0.9695\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 1s 304ms/step - loss: 0.0725 - acc: 0.9695\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 2s 411ms/step - loss: 0.0725 - acc: 0.9695\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 2s 386ms/step - loss: 0.0724 - acc: 0.9695\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.0723 - acc: 0.9695\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.0722 - acc: 0.9695\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 1s 236ms/step - loss: 0.0722 - acc: 0.9695\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 1s 236ms/step - loss: 0.0721 - acc: 0.9695\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 1s 230ms/step - loss: 0.0721 - acc: 0.9700\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 1s 232ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 1s 231ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 2s 402ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 2s 383ms/step - loss: 0.0719 - acc: 0.9695\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 2s 269ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 1s 256ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.0720 - acc: 0.9695\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.0720 - acc: 0.9695\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4f2d12b60>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем текст с помощью нашей модели"
      ],
      "metadata": {
        "id": "b1pqdfAe4dES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, size, sentence):\n",
        "  start = sentence\n",
        "  for i in range(0, size):\n",
        "    v = vectorizer([start])[0]\n",
        "    out1 = tf.one_hot(v, vocab_size)\n",
        "    out1 = tf.reshape(out1, [1, out1.shape[0], out1.shape[1]])\n",
        "    logits = model(out1)[0][-1]\n",
        "    nc = tf.argmax(logits)\n",
        "    print(vectorizer.get_vocabulary()[nc], end= \" \")\n",
        "    if vectorizer.get_vocabulary()[nc] == \"eos\" or vectorizer.get_vocabulary()[nc] == \"\": break\n",
        "    start = vectorizer.get_vocabulary()[nc]"
      ],
      "metadata": {
        "id": "U9Dsz77H42hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_soft(model, size, sentence, temperature=1.0):\n",
        "   start = sentence\n",
        "   for i in range(0, size):\n",
        "     v = vectorizer([start])[0]\n",
        "     out1 = tf.one_hot(v, vocab_size)\n",
        "     out1 = tf.reshape(out1, [1, out1.shape[0], out1.shape[1]])\n",
        "     logits = model(out1)[0][-1]\n",
        "     probs = tf.exp(tf.math.log(logits) / temperature).numpy().astype(np.float64)\n",
        "     probs = probs / np.sum(probs)\n",
        "     nc = np.argmax(np.random.multinomial(1, probs, 1))\n",
        "     print(vectorizer.get_vocabulary()[nc], end= \" \")\n",
        "     if vectorizer.get_vocabulary()[nc] == \"eos\" or vectorizer.get_vocabulary()[nc] == \"\": break\n",
        "     start = vectorizer.get_vocabulary()[nc]"
      ],
      "metadata": {
        "id": "T7_T3uxf8eIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(60)"
      ],
      "metadata": {
        "id": "2jL19tW4Bjcr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87ca89f1-1d69-49b5-d086-0fa3e31610a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    start_time  end_time                                               text  \\\n",
              "0       58.559    61.602   BOY: All right, everyone! This... is a stick-up!   \n",
              "1       61.687    63.354                                Don't anybody move!   \n",
              "2       64.398    66.482                              Now, empty that safe!   \n",
              "3       68.318    71.612        Ooh-hoo-hoo! Money, money, money! (KISSING)   \n",
              "4       71.697    74.031            Stop it! Stop it, you mean, old potato!   \n",
              "5       74.116    77.160        Quiet, Bo Peep, or your sheep get run over!   \n",
              "6       77.327    79.328                                Help! Baa! Help us!   \n",
              "7       79.413    82.957       Oh, no, not my sheep! Somebody do something!   \n",
              "8       89.339    92.425                      VOICE BOX: Reach for the sky!   \n",
              "9       92.509    94.677                             Oh, no! Sheriff Woody!   \n",
              "10      94.762    97.513               I'm here to stop you, One-Eyed Bart.   \n",
              "11      97.848    99.974                     Doh! How'd you know it was me?   \n",
              "12     100.058   101.768                        Are you gonna come quietly?   \n",
              "13     101.852   103.060                       You can't touch me, Sheriff!   \n",
              "14     103.187   106.439  I brought my attack dog with the built-in forc...   \n",
              "15     106.523   110.318  Well, I brought my dinosaur who eats force-fie...   \n",
              "16     110.402   112.904               -(GROWLING) -Yipe, yipe, yipe, yipe!   \n",
              "17     112.988   114.697                        You're going to jail, Bart!   \n",
              "18     114.782   116.866            Say goodbye to the wife and Tater Tots.   \n",
              "19     116.950   118.993                               (GIGGLING EXCITEDLY)   \n",
              "20     120.454   122.038                                         (LAUGHING)   \n",
              "21     122.372   126.083                 (BABY SQUEALING, LAUGHING, COOING)   \n",
              "22     126.168   128.503                    You saved the day again, Woody.   \n",
              "23     128.587   131.130              VOlCE BOX: You're my favorite deputy.   \n",
              "24     131.215   133.716                          You've got a friend in me   \n",
              "25     135.594   137.720                          You've got a friend in me   \n",
              "26     137.804   139.555              Come on, let's wrangle up the cattle.   \n",
              "27     139.640   142.767                    When the road looks rough ahead   \n",
              "28     142.851   146.646   And you're miles and miles from your nice, wa...   \n",
              "29     146.730   147.980                              Round 'em up, cowboy!   \n",
              "30     148.065   150.858               Just remember what your old pal said   \n",
              "31     150.943   153.861                     Boy, you've got a friend in me   \n",
              "32     153.946   154.987                                           Yee-haw!   \n",
              "33     155.072   157.949                    Yeah, you've got a friend in me   \n",
              "34     158.033   159.992                                       Hey, cowboy!   \n",
              "35     160.077   162.912   Some other folks might be a little bit smarte...   \n",
              "36     162.996   163.996                                        (WHINNYING)   \n",
              "37     164.081   165.831                               Big and stronger too   \n",
              "38     165.916   166.958                                    Come on, Woody.   \n",
              "39     167.042   168.042                                              Maybe   \n",
              "40     168.126   173.339   But none of them will ever love you the way l do   \n",
              "41     173.423   175.883                   - lt's me and you, boy -(LAUGHS)   \n",
              "42     175.968   177.218                             And as the years go by   \n",
              "43     177.302   179.387                             Whoa! Whoa! (LAUGHING)   \n",
              "44     179.471   182.431                      Our friendship will never die   \n",
              "45     183.517   184.517                                              Whoo!   \n",
              "46     184.601   186.561                  You're gonna see lt's our destiny   \n",
              "47     186.645   188.020                                         (LAUGHING)   \n",
              "48     188.772   190.690                          You've got a friend in me   \n",
              "49     190.774   191.899                                         All right!   \n",
              "50     191.984   194.402                    Yeah, you've got a friend in me   \n",
              "51     194.486   195.903                                             Score!   \n",
              "52     196.780   199.448              - You got a friend in me  -Wow! Cool!   \n",
              "53     199.533   202.660  -MOM: What do you think? -Oh, this looks great...   \n",
              "54     202.744   204.036                              Okay, birthday boy...   \n",
              "55     204.121   206.163      We saw that at the store! I asked you for it!   \n",
              "56     206.248   207.999  -I hope I have enough places. -Wow, look at th...   \n",
              "57     208.083   210.001        -One, two... Four. -Oh, my gosh, you got...   \n",
              "58     210.085   212.795  -Yeah, I think that's gonna be enough. -Could ...   \n",
              "59     212.879   214.797            -Well, sure! We can leave it up. -Yeah!   \n",
              "\n",
              "      imdb_id  word_count  \n",
              "0   tt0114709           8  \n",
              "1   tt0114709           3  \n",
              "2   tt0114709           4  \n",
              "3   tt0114709           5  \n",
              "4   tt0114709           8  \n",
              "5   tt0114709           9  \n",
              "6   tt0114709           4  \n",
              "7   tt0114709           8  \n",
              "8   tt0114709           6  \n",
              "9   tt0114709           4  \n",
              "10  tt0114709           7  \n",
              "11  tt0114709           7  \n",
              "12  tt0114709           5  \n",
              "13  tt0114709           5  \n",
              "14  tt0114709          10  \n",
              "15  tt0114709           9  \n",
              "16  tt0114709           5  \n",
              "17  tt0114709           5  \n",
              "18  tt0114709           8  \n",
              "19  tt0114709           2  \n",
              "20  tt0114709           1  \n",
              "21  tt0114709           4  \n",
              "22  tt0114709           6  \n",
              "23  tt0114709           6  \n",
              "24  tt0114709           7  \n",
              "25  tt0114709           7  \n",
              "26  tt0114709           7  \n",
              "27  tt0114709           7  \n",
              "28  tt0114709          11  \n",
              "29  tt0114709           4  \n",
              "30  tt0114709           8  \n",
              "31  tt0114709           8  \n",
              "32  tt0114709           1  \n",
              "33  tt0114709           8  \n",
              "34  tt0114709           2  \n",
              "35  tt0114709          13  \n",
              "36  tt0114709           1  \n",
              "37  tt0114709           5  \n",
              "38  tt0114709           3  \n",
              "39  tt0114709           2  \n",
              "40  tt0114709          13  \n",
              "41  tt0114709           7  \n",
              "42  tt0114709           7  \n",
              "43  tt0114709           3  \n",
              "44  tt0114709           6  \n",
              "45  tt0114709           1  \n",
              "46  tt0114709           7  \n",
              "47  tt0114709           1  \n",
              "48  tt0114709           7  \n",
              "49  tt0114709           2  \n",
              "50  tt0114709           8  \n",
              "51  tt0114709           1  \n",
              "52  tt0114709          10  \n",
              "53  tt0114709          10  \n",
              "54  tt0114709           3  \n",
              "55  tt0114709          11  \n",
              "56  tt0114709          12  \n",
              "57  tt0114709           8  \n",
              "58  tt0114709          15  \n",
              "59  tt0114709           8  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1dd94a88-fb3c-4599-b5a8-7906a10503f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>text</th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58.559</td>\n",
              "      <td>61.602</td>\n",
              "      <td>BOY: All right, everyone! This... is a stick-up!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61.687</td>\n",
              "      <td>63.354</td>\n",
              "      <td>Don't anybody move!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.398</td>\n",
              "      <td>66.482</td>\n",
              "      <td>Now, empty that safe!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>68.318</td>\n",
              "      <td>71.612</td>\n",
              "      <td>Ooh-hoo-hoo! Money, money, money! (KISSING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71.697</td>\n",
              "      <td>74.031</td>\n",
              "      <td>Stop it! Stop it, you mean, old potato!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>74.116</td>\n",
              "      <td>77.160</td>\n",
              "      <td>Quiet, Bo Peep, or your sheep get run over!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>77.327</td>\n",
              "      <td>79.328</td>\n",
              "      <td>Help! Baa! Help us!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>79.413</td>\n",
              "      <td>82.957</td>\n",
              "      <td>Oh, no, not my sheep! Somebody do something!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>89.339</td>\n",
              "      <td>92.425</td>\n",
              "      <td>VOICE BOX: Reach for the sky!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>92.509</td>\n",
              "      <td>94.677</td>\n",
              "      <td>Oh, no! Sheriff Woody!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>94.762</td>\n",
              "      <td>97.513</td>\n",
              "      <td>I'm here to stop you, One-Eyed Bart.</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>97.848</td>\n",
              "      <td>99.974</td>\n",
              "      <td>Doh! How'd you know it was me?</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>100.058</td>\n",
              "      <td>101.768</td>\n",
              "      <td>Are you gonna come quietly?</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>101.852</td>\n",
              "      <td>103.060</td>\n",
              "      <td>You can't touch me, Sheriff!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>103.187</td>\n",
              "      <td>106.439</td>\n",
              "      <td>I brought my attack dog with the built-in forc...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>106.523</td>\n",
              "      <td>110.318</td>\n",
              "      <td>Well, I brought my dinosaur who eats force-fie...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>110.402</td>\n",
              "      <td>112.904</td>\n",
              "      <td>-(GROWLING) -Yipe, yipe, yipe, yipe!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>112.988</td>\n",
              "      <td>114.697</td>\n",
              "      <td>You're going to jail, Bart!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>114.782</td>\n",
              "      <td>116.866</td>\n",
              "      <td>Say goodbye to the wife and Tater Tots.</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>116.950</td>\n",
              "      <td>118.993</td>\n",
              "      <td>(GIGGLING EXCITEDLY)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>120.454</td>\n",
              "      <td>122.038</td>\n",
              "      <td>(LAUGHING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>122.372</td>\n",
              "      <td>126.083</td>\n",
              "      <td>(BABY SQUEALING, LAUGHING, COOING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>126.168</td>\n",
              "      <td>128.503</td>\n",
              "      <td>You saved the day again, Woody.</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>128.587</td>\n",
              "      <td>131.130</td>\n",
              "      <td>VOlCE BOX: You're my favorite deputy.</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>131.215</td>\n",
              "      <td>133.716</td>\n",
              "      <td>You've got a friend in me</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>135.594</td>\n",
              "      <td>137.720</td>\n",
              "      <td>You've got a friend in me</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>137.804</td>\n",
              "      <td>139.555</td>\n",
              "      <td>Come on, let's wrangle up the cattle.</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>139.640</td>\n",
              "      <td>142.767</td>\n",
              "      <td>When the road looks rough ahead</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>142.851</td>\n",
              "      <td>146.646</td>\n",
              "      <td>And you're miles and miles from your nice, wa...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>146.730</td>\n",
              "      <td>147.980</td>\n",
              "      <td>Round 'em up, cowboy!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>148.065</td>\n",
              "      <td>150.858</td>\n",
              "      <td>Just remember what your old pal said</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>150.943</td>\n",
              "      <td>153.861</td>\n",
              "      <td>Boy, you've got a friend in me</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>153.946</td>\n",
              "      <td>154.987</td>\n",
              "      <td>Yee-haw!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>155.072</td>\n",
              "      <td>157.949</td>\n",
              "      <td>Yeah, you've got a friend in me</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>158.033</td>\n",
              "      <td>159.992</td>\n",
              "      <td>Hey, cowboy!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>160.077</td>\n",
              "      <td>162.912</td>\n",
              "      <td>Some other folks might be a little bit smarte...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>162.996</td>\n",
              "      <td>163.996</td>\n",
              "      <td>(WHINNYING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>164.081</td>\n",
              "      <td>165.831</td>\n",
              "      <td>Big and stronger too</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>165.916</td>\n",
              "      <td>166.958</td>\n",
              "      <td>Come on, Woody.</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>167.042</td>\n",
              "      <td>168.042</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>168.126</td>\n",
              "      <td>173.339</td>\n",
              "      <td>But none of them will ever love you the way l do</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>173.423</td>\n",
              "      <td>175.883</td>\n",
              "      <td>- lt's me and you, boy -(LAUGHS)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>175.968</td>\n",
              "      <td>177.218</td>\n",
              "      <td>And as the years go by</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>177.302</td>\n",
              "      <td>179.387</td>\n",
              "      <td>Whoa! Whoa! (LAUGHING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>179.471</td>\n",
              "      <td>182.431</td>\n",
              "      <td>Our friendship will never die</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>183.517</td>\n",
              "      <td>184.517</td>\n",
              "      <td>Whoo!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>184.601</td>\n",
              "      <td>186.561</td>\n",
              "      <td>You're gonna see lt's our destiny</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>186.645</td>\n",
              "      <td>188.020</td>\n",
              "      <td>(LAUGHING)</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>188.772</td>\n",
              "      <td>190.690</td>\n",
              "      <td>You've got a friend in me</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>190.774</td>\n",
              "      <td>191.899</td>\n",
              "      <td>All right!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>191.984</td>\n",
              "      <td>194.402</td>\n",
              "      <td>Yeah, you've got a friend in me</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>194.486</td>\n",
              "      <td>195.903</td>\n",
              "      <td>Score!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>196.780</td>\n",
              "      <td>199.448</td>\n",
              "      <td>- You got a friend in me  -Wow! Cool!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>199.533</td>\n",
              "      <td>202.660</td>\n",
              "      <td>-MOM: What do you think? -Oh, this looks great...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>202.744</td>\n",
              "      <td>204.036</td>\n",
              "      <td>Okay, birthday boy...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>204.121</td>\n",
              "      <td>206.163</td>\n",
              "      <td>We saw that at the store! I asked you for it!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>206.248</td>\n",
              "      <td>207.999</td>\n",
              "      <td>-I hope I have enough places. -Wow, look at th...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>208.083</td>\n",
              "      <td>210.001</td>\n",
              "      <td>-One, two... Four. -Oh, my gosh, you got...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>210.085</td>\n",
              "      <td>212.795</td>\n",
              "      <td>-Yeah, I think that's gonna be enough. -Could ...</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>212.879</td>\n",
              "      <td>214.797</td>\n",
              "      <td>-Well, sure! We can leave it up. -Yeah!</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1dd94a88-fb3c-4599-b5a8-7906a10503f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1dd94a88-fb3c-4599-b5a8-7906a10503f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1dd94a88-fb3c-4599-b5a8-7906a10503f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, 20, [\"here\"])"
      ],
      "metadata": {
        "id": "gW3tFkiL8Hc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa72c38-d3c7-43af-c577-367593b9c2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "woody [UNK]  "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"empty\", \"boring\", \"happy\", \"bridge\",  \"you\", \"where\", \"when\", \"take\", \"us\", \"want\", \"remember\", \"friendship\", \"coming\", \"expected\", \"make\", \"do\", \"all\", \"birthday\", \"chance\"]\n",
        "temperature_rates = [1, 1.2, 1.4, 1.6]\n",
        "for word in start_words:\n",
        "  for temperature in temperature_rates:\n",
        "    print(\"[START WORD]: \", word, \" [TEMPERATURE]:\", temperature)\n",
        "    generate_soft(model, 20, [word], temperature=temperature)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNm5JKt9cbHh",
        "outputId": "6a822b20-4ca8-41f8-cade-f96aa2b7a5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START WORD]:  empty  [TEMPERATURE]: 1\n",
            "that move as the cousin breathe ouch [UNK] come on [UNK] come on oh youre going for crying tai [UNK] \n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.2\n",
            "first [UNK] come on wrap as [UNK] [UNK] box gather everyone for hey [UNK] come on come on box be \n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.4\n",
            "no [UNK] howd me dreadful tickets it you can andrews [UNK] you can be happy ha [UNK] box ishe for \n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.6\n",
            "teams amazing for forgive journey friends tree bottle brief hates kong chattering loving arliss box wagon glorious ernie donald mcmurphy \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1\n",
            "[UNK]  \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.2\n",
            "price [UNK]  \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.4\n",
            "youve got it promises happened [UNK]  \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.6\n",
            "serial no [UNK] money moved moms all right here woody hot charge field blows state bad roxanne 90 units gonna \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1\n",
            "ha got a go as [UNK] you got it  \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.2\n",
            "ha want hey [UNK] howd you [UNK] come on running [UNK] for going for not now go can ya evidence \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.4\n",
            "ha wow who always track session book iman its recall [UNK] sarge gentle dreaming their barking avoid spend grandson spoken \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.6\n",
            "too staff meeting everybody hear settle paradise [UNK] i didnt its walking valuable youre m promises theyve shrink ithat reese \n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1\n",
            "jenny gonna today hey who none of the hey sarge have can be happy ha bad news  \n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.2\n",
            "missed [UNK] howd you saved the was it  \n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.4\n",
            "josh chest reporter cheated isi dollars tricks loyal hurting wouldve immediately calm got a simple island david cole clear ryan \n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.6\n",
            "[UNK]  \n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1\n",
            "[UNK] [UNK] money money money draw lowrey box youre going for crying fat oh no [UNK] for it 3 ow \n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.2\n",
            "can eaten prison bad sleeping shirt cowboy deep medical [UNK] 3 or drank [UNK] bad news wolves moments [UNK] howd \n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.4\n",
            "cant accounts here shore got a got a gather okay [UNK] come on trace curse served mrs louis arrangement box \n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.6\n",
            "[UNK] [UNK]  \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1\n",
            "is [UNK]  \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.2\n",
            "smaller toys [UNK] come on lets bye delighted hey hey [UNK] money  \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.4\n",
            "wherever demands uh oh no [UNK] sealed investigation i hope who can as can checking uhhuh 19 sarge source planning \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.6\n",
            "youve think im going it sore whatre bored liar sarge youre going shower is deadly a charges conduct unfair general \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1\n",
            "the appeal gather everyone come on woody youve got it  \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.2\n",
            "the quickly cowboy for [UNK]  \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.4\n",
            "the woody beaten gave only carrie whip hey sarge martha cowboy  \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.6\n",
            "the hey respond [UNK]  \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1\n",
            "rachel empty up youre going for crying cowboy [UNK]  \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.2\n",
            "mancub nap passes [UNK] bad shirt can be happy ha testing screwing as the later [UNK] howd you [UNK] box \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.4\n",
            "it here bathroom go patience ow me and as [UNK] bad princess michele jesus howd you got you can cellar \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.6\n",
            "cairo violet kicks ate works come on boog been yet me tuesday dads boyfriend international brush kick back [UNK] come \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1\n",
            "[UNK] for [UNK]  \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.2\n",
            "lazy yeah youve a cant robbed [UNK] can be happy here think im going for [UNK] money i’ll peoples was \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.4\n",
            "chuckles [UNK] oh youre going here illegal go mysterious hey sarge silver mick these gather everyone wears actor brought inow \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.6\n",
            "bucket snake dickie virus flag aunt mistakes money help [UNK] ow box has kenny wedding regret press youd until waiter \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1\n",
            "[UNK] [UNK]  \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.2\n",
            "[UNK] [UNK] box youre going box saved the sheep [UNK] for oh youre gonna might be happy ha youre gonna \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.4\n",
            "floor duke already soccer im here oath iis fbi  \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.6\n",
            "machine dragged thumb moves rubber got it  \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1\n",
            "im going i didnt know can be happy ha can be happy ha no [UNK] hey [UNK]  \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.2\n",
            "im going for the boomer hey who brought my birthday come on walking deeply lawyer [UNK] who what do explosion \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.4\n",
            "im going bad shower bored [UNK] 3 its hey who first [UNK] up stopped darling polite hated falls francis hey \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.6\n",
            "im going outer session contrary today you saved the spent god field youve got me thatd bandits serving downi mmmhmm \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1\n",
            "will i wanted to [UNK]  \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.2\n",
            "will holiness cant touch eli action can brer drugs its media wood bottom excited escort principal cable sheriff woody hey \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.4\n",
            "will another impossible film virginia lt´s his [UNK]  \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.6\n",
            "shopping drink lesson madam crew ready boarding [UNK]  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1\n",
            "misery me  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.2\n",
            "department digging i think im going for cowboy  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.4\n",
            "tradition youve got everybody hear understands isome sarge brought my hang deal belong wrap regret fish waitin rome tough  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.6\n",
            "fans dirt it headquarters surprise graduate sheriff it come on accounts money alright refuse faggot locker am nina baseball enright \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1\n",
            "uh oh no [UNK] for crying tracks jerry [UNK] come on woody hey sarge have jamie draw hey [UNK] hey \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.2\n",
            "desert box youre going for 3 where is spanish i didnt know it hey who gonna see yeahi arei as \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.4\n",
            "hadnt copy oh youre gonna see were bother dame everythings known here unhappy manager i hope [UNK] come on kansas \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.6\n",
            "wing [UNK] box thornton everybody hear jimmy commercial iwhy speaking virginia ido familys tooi 60 joke steak gate cowboy for \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1\n",
            "bad news it committee  \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.2\n",
            " \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.4\n",
            "victim whore cowboy  \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.6\n",
            "francisco dozen relationship sheriff come on warm spend ithey outi m yards cries was hal fix sort exhausted towards rubber \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1\n",
            "you got its [UNK] bad muncie dolphin can cowboy  \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.2\n",
            "i didnt know it  \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.4\n",
            "bad answers iwhats authorities howd you draw [UNK] ow [UNK] howd clean profit yellow avoid running [UNK] box flew iget \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.6\n",
            "miles gonna marcaillou dude spoiled youve can be happy ha staff meeting first running [UNK] i didnt morelli should testimony \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1\n",
            "right here woody come on [UNK] box youre going for for guest mother none of dice box little [UNK] [UNK] \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.2\n",
            "right here going who got it  \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.4\n",
            "right here setting milo drinking worries color killed dive desperate not now where is mexico twelve themi getting motor [UNK] \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.6\n",
            "right [UNK] bad news gonna en juice oh hey who bitches for bite glen care him starving hallie cowboy yall \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1\n",
            "boy youve got a staff meeting everybody hear me  \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.2\n",
            "[UNK] [UNK] you birthday you got it hey cowboy  \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.4\n",
            "uh oh hey [UNK] come on was sulu [UNK] ow and as [UNK] money hey who [UNK]  \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.6\n",
            "3 hang drowned receive page birdie god my day add idiot run car return money money come box bored suffering \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1\n",
            "[UNK] howd you [UNK] makin bitter i wanted to  \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.2\n",
            "follow for wet bad news  \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.4\n",
            "salt have light sick throughout generous pound youve them gorgeous farmers breath stolen ow [UNK]  \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.6\n",
            "bethany who wanted to painting guile sulu mostly got a sensitive criminals plus youve got a miami money money discovered \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим многослойную STLM"
      ],
      "metadata": {
        "id": "7aspU-5zfVPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multilayer_stlm = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(None, 5000)),\n",
        "    tf.keras.layers.Masking(mask_value=0),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "multilayer_stlm.build((None, None, 5000))\n",
        "multilayer_stlm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz6CpFdgfaAh",
        "outputId": "ba41a091-c1da-4852-d462-c4a249965f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_4 (Masking)         (None, None, 5000)        0         \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, None, 128)         2626048   \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              (None, None, 128)         131584    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, None, 5000)        645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,534,216\n",
            "Trainable params: 3,534,216\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multilayer_stlm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "index_=0\n",
        "X_, Y_ = prepare_data(np.array(get_text(data.loc[index * batch_size:index * batch_size + batch_size])))\n",
        "multilayer_stlm.fit(X_, Y_,epochs=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C18yRrhHfprs",
        "outputId": "ef43d4d6-ca8a-433b-d1a7-d5080de85c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 16s 297ms/step - loss: 0.6124 - acc: 0.8532\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 2s 296ms/step - loss: 0.6116 - acc: 0.8537\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 2s 291ms/step - loss: 0.6113 - acc: 0.8553\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 2s 300ms/step - loss: 0.6114 - acc: 0.8548\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 1s 286ms/step - loss: 0.6110 - acc: 0.8537\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 2s 299ms/step - loss: 0.6111 - acc: 0.8532\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 2s 434ms/step - loss: 0.6105 - acc: 0.8537\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 3s 494ms/step - loss: 0.6104 - acc: 0.8543\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 2s 300ms/step - loss: 0.6102 - acc: 0.8548\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 2s 304ms/step - loss: 0.6101 - acc: 0.8543\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 2s 301ms/step - loss: 0.6100 - acc: 0.8548\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 2s 299ms/step - loss: 0.6099 - acc: 0.8543\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 2s 297ms/step - loss: 0.6097 - acc: 0.8543\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 1s 290ms/step - loss: 0.6095 - acc: 0.8548\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 2s 334ms/step - loss: 0.6094 - acc: 0.8543\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.6093 - acc: 0.8553\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 2s 415ms/step - loss: 0.6091 - acc: 0.8553\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 2s 289ms/step - loss: 0.6090 - acc: 0.8537\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 2s 285ms/step - loss: 0.6090 - acc: 0.8537\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 2s 294ms/step - loss: 0.6088 - acc: 0.8543\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 2s 306ms/step - loss: 0.6087 - acc: 0.8548\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 2s 294ms/step - loss: 0.6086 - acc: 0.8548\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 2s 300ms/step - loss: 0.6085 - acc: 0.8543\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.6084 - acc: 0.8537\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 3s 489ms/step - loss: 0.6083 - acc: 0.8553\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 1s 286ms/step - loss: 0.6081 - acc: 0.8563\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 2s 290ms/step - loss: 0.6081 - acc: 0.8558\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 2s 286ms/step - loss: 0.6079 - acc: 0.8532\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 1s 291ms/step - loss: 0.6077 - acc: 0.8548\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 2s 289ms/step - loss: 0.6076 - acc: 0.8558\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 2s 307ms/step - loss: 0.6075 - acc: 0.8558\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 2s 393ms/step - loss: 0.6074 - acc: 0.8563\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.6073 - acc: 0.8553\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 2s 332ms/step - loss: 0.6071 - acc: 0.8563\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 2s 304ms/step - loss: 0.6070 - acc: 0.8568\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.6069 - acc: 0.8574\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 2s 303ms/step - loss: 0.6069 - acc: 0.8574\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 1s 289ms/step - loss: 0.6067 - acc: 0.8574\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 2s 292ms/step - loss: 0.6067 - acc: 0.8558\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 2s 319ms/step - loss: 0.6067 - acc: 0.8558\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 3s 495ms/step - loss: 0.6064 - acc: 0.8558\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 2s 411ms/step - loss: 0.6065 - acc: 0.8553\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 1s 290ms/step - loss: 0.6063 - acc: 0.8543\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.6062 - acc: 0.8548\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.6061 - acc: 0.8548\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 2s 298ms/step - loss: 0.6060 - acc: 0.8553\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 2s 292ms/step - loss: 0.6058 - acc: 0.8553\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 2s 291ms/step - loss: 0.6057 - acc: 0.8558\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 2s 474ms/step - loss: 0.6056 - acc: 0.8558\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 3s 490ms/step - loss: 0.6055 - acc: 0.8558\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 2s 299ms/step - loss: 0.6058 - acc: 0.8553\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 1s 288ms/step - loss: 0.6054 - acc: 0.8558\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 2s 289ms/step - loss: 0.6054 - acc: 0.8553\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 2s 296ms/step - loss: 0.6053 - acc: 0.8553\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 2s 300ms/step - loss: 0.6054 - acc: 0.8553\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.6051 - acc: 0.8553\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.6051 - acc: 0.8553\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 3s 490ms/step - loss: 0.6048 - acc: 0.8553\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 2s 381ms/step - loss: 0.6048 - acc: 0.8543\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.6047 - acc: 0.8543\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 2s 293ms/step - loss: 0.6046 - acc: 0.8537\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 2s 294ms/step - loss: 0.6044 - acc: 0.8543\n",
            "Epoch 63/200\n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.5887 - acc: 0.8618"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-cf8a41a00eb1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#X, Y = prepare_data(np.array(get_text(data.loc[500:1500])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmultilayer_stlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(multilayer_stlm, 20, [\"think\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ist8XJYmBbH",
        "outputId": "2cfb22da-5f21-4f9b-9423-e572f2984db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_soft(multilayer_stlm, 20, [\"never\"], temperature=1.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw7Gc_NPmLbJ",
        "outputId": "12eb8eac-cbab-4ad2-d91f-b752dffdef63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going remember im explosion bad anybody 3 its [UNK]  "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"empty\", \"boring\", \"happy\", \"bridge\"  \"you\", \"where\", \"when\", \"take\", \"us\", \"want\", \"remember\", \"friendship\", \"coming\", \"expected\", \"make\", \"do\", \"all\", \"birthday\", \"chance\"]\n",
        "temperature_rates = [1, 1.2, 1.4, 1.6]\n",
        "for word in start_words:\n",
        "  for temperature in temperature_rates:\n",
        "    print(\"[START WORD]: \", word, \" [TEMPERATURE]:\", temperature)\n",
        "    generate_soft(multilayer_stlm, 20, [word], temperature=temperature)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOOq7thaSjMG",
        "outputId": "9e70584a-18b0-41f9-e6a8-d13ceeea2689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START WORD]:  empty  [TEMPERATURE]: 1\n",
            "[UNK] whoa hey [UNK] [UNK] a have come [UNK] ow for [UNK] 3 running what hear what me [UNK]  \n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.2\n",
            "gonna have box no ha sure everybody in box [UNK] bad [UNK] [UNK] come youre running im going now oh \n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.4\n",
            "[UNK]  \n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.6\n",
            "we oh woody me be [UNK] face meeting it for lets bo shh  \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1\n",
            "got want im going i wanted first im going remember im here anybody on [UNK] howd [UNK] youre going got \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.2\n",
            "meeting got going remember what hear what i think  \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.4\n",
            "got where thank  \n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.6\n",
            "be gonna toys [UNK] oh [UNK] for [UNK] up [UNK] right here  \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1\n",
            "[UNK] the birthday [UNK] for on [UNK] it happy  \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.2\n",
            "right going meeting friendship  \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.4\n",
            "laughing this everybody hear now it youre come on [UNK] on box closes for [UNK] come and as on were \n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.6\n",
            " \n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1\n",
            "gonna  \n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1.2\n",
            "bad shh cowboy i hope [UNK] box anybody its little little em staff [UNK] oh box shh [UNK] come bad \n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1.4\n",
            "oh come for it for [UNK] it box em for come on  \n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1.6\n",
            "[UNK] doin be [UNK] going what do everybody im own or that  \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1\n",
            "[UNK] it gonna  \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.2\n",
            " \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.4\n",
            " \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.6\n",
            "whoa and to meeting got where  \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1\n",
            "youre box anybody [UNK] its em  \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.2\n",
            " \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.4\n",
            "hey who bad gonna right where  \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.6\n",
            " \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1\n",
            "[UNK] running what hear youve got going im hers it for on gonna [UNK] not now go everybody hear gather \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.2\n",
            "got got [UNK] on be saved im here [UNK] a right empty for come can my youve can money [UNK] \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.4\n",
            "gonna hey cowboy remember remember im here and as  \n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.6\n",
            "empty box box [UNK] gonna toys [UNK] my youve get got meeting gonna [UNK] its birthday bad shh stronger cant \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1\n",
            "got going what do friendship to for on for for hey em  \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.2\n",
            "i brought do get hear what hear now go birthday was  \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.4\n",
            "bo and here em know going as destiny empty go bad  \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.6\n",
            "got be got yeah youve this for ow the move meeting gonna shh meeting and miles friendship we saw [UNK] \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1\n",
            "i hope cant  \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.2\n",
            "bo whoa lady got oh everybody and as on my oh hey [UNK] howd my  \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.4\n",
            "me me box anybody the right what do gonna its hey birthday hey moved for [UNK]  \n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.6\n",
            "news bo  \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1\n",
            "im youre everybody do gonna right here laughing  \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.2\n",
            "im here [UNK] going just gather youve meeting got  \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.4\n",
            "[UNK] 3 gonna no me friendship bart as  \n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.6\n",
            "what hear be got me 3 oh oh box anybody it [UNK] you cant today all you friendship the em \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1\n",
            "closes got box whoa closes got here shh laughing  \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.2\n",
            "do where  \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.4\n",
            "whoa hey [UNK] cool a right meeting got [UNK]  \n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.6\n",
            "you youve cant destiny youre you [UNK] money the anybody [UNK] [UNK] box hole saved  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1\n",
            "[UNK]  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.2\n",
            "meeting i think hey [UNK] right can no gonna [UNK]  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.4\n",
            "to on the hey got it [UNK] [UNK] on draw bo [UNK] howd howd  \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.6\n",
            "and empty running im buying got woody got woody safe tellin im going gather now everyone here little you be \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1\n",
            "go [UNK]  \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.2\n",
            "now can box and it to happy anybody birthday [UNK] got oh on [UNK] 3 box  \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.4\n",
            "youve right it going now here [UNK] [UNK] running remember im going what think siren [UNK] oh who here laughing \n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.6\n",
            " \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1\n",
            "got box [UNK] [UNK]  \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.2\n",
            "got its [UNK] ow [UNK] my going everyone got [UNK] everybody hear meeting where [UNK] ow come [UNK] bad right \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.4\n",
            "for no box anybody gonna sarge youre going bo [UNK]  \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.6\n",
            "empty going news im going go somebodys me going youve mistakes attend got running gather brat where fits that [UNK] \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1\n",
            "for it howd  \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.2\n",
            "cant closes it for it and youre oh everybody hear youve got here hey sarge hey cowboy what do got \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.4\n",
            "got as  \n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.6\n",
            "meeting bad you saved im as you oh 3 bad anybody go right cant anybody it all closes gonna lady \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1\n",
            " \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.2\n",
            "motor empty oh hey em box closes got its toys sarge oh woody got the shh anybody oh no box \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.4\n",
            "cowboy i hope [UNK] news i [UNK] me youve meeting oh the em oh no everybody hear youve all move \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.6\n",
            " \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1\n",
            "howd youre [UNK] oh thank where  \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.2\n",
            "[UNK] oh to ha empty you [UNK] meeting im here [UNK] box no oh the sarge you [UNK]  \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.4\n",
            "going im mean got box lady meeting be saved i was right [UNK] its [UNK] [UNK]  \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.6\n",
            "for meeting the whoa [UNK] [UNK] woody empty gonna toys on it news leave hear i scared [UNK] youre [UNK] \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1\n",
            "youve can howd for no box lady got [UNK] you got friendship right gonna [UNK]  \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.2\n",
            "check sarge box [UNK] it hey you cant [UNK] [UNK] got meeting cant right yeah got bad  \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.4\n",
            "as this news as  \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.6\n",
            "here toys draw what hear all its do happy its [UNK] gather remember im duck youre ow [UNK] my come \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Модели с посимвольной токенизацией**"
      ],
      "metadata": {
        "id": "PwI205nKwWGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
        "tokenizer.fit_on_texts(list(get_text(data.loc[0:10000])))"
      ],
      "metadata": {
        "id": "nI608ycaWA21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eos_token = len(tokenizer.word_index)+1\n",
        "tokenizer.word_index['<eos>'] = eos_token\n",
        "\n",
        "vocab_size = eos_token + 1\n",
        "maxlen = data.loc[:100000][\"text\"].apply(lambda x: len(x)).max()\n",
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFS5eL-oWBAX",
        "outputId": "7cb330f9-bcf7-4982-f7a4-e29d3a464b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_simple_rnn = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Masking(input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.SimpleRNN(128,return_sequences=True,input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size,activation='softmax'))\n",
        "])\n",
        "symbol_simple_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEn_0HpEWBGO",
        "outputId": "d1a99f15-ee35-4fce-e9a1-5606f16106af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking (Masking)           (None, 312, 83)           0         \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 312, 128)          27136     \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 312, 83)          10707     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,843\n",
            "Trainable params: 37,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_symbol(x):\n",
        "  x = tokenizer.texts_to_sequences(x)\n",
        "  y = [i[1:] + [eos_token] for i in x]\n",
        "  x = tf.keras.preprocessing.sequence.pad_sequences(x,padding='post',maxlen=maxlen)\n",
        "  y = tf.keras.preprocessing.sequence.pad_sequences(y,padding='post',maxlen=maxlen)\n",
        "  out1 = tf.one_hot(x, vocab_size)\n",
        "  out2 = tf.one_hot(y, vocab_size)\n",
        "  return out1, out2"
      ],
      "metadata": {
        "id": "jAkCwY11X-tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В процессе обучения я уменьшаю темп обучения в 2 раза, если лосс перестаёт монотонно падать. По-хорошему для обучения стоило подобрать LRScheduler, который менят темп обучения по некоторому правилу."
      ],
      "metadata": {
        "id": "IQ96ucOWJcvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_simple_rnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "x1, y1 = prepare_for_symbol(list(get_text(data.loc[:1000])))\n",
        "symbol_simple_rnn.fit(x1, y1, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVLl7gxaY-Pe",
        "outputId": "94f40e57-bdf6-4169-9f78-c35034328bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 5s 131ms/step - loss: 0.1063 - acc: 0.9696\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 5s 145ms/step - loss: 0.1040 - acc: 0.9708\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1033 - acc: 0.9712\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.1030 - acc: 0.9713\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 5s 150ms/step - loss: 0.1027 - acc: 0.9714\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1025 - acc: 0.9716\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.1024 - acc: 0.9715\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 5s 154ms/step - loss: 0.1026 - acc: 0.9715\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.1024 - acc: 0.9716\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1022 - acc: 0.9718\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 5s 153ms/step - loss: 0.1021 - acc: 0.9716\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1020 - acc: 0.9718\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1020 - acc: 0.9717\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.1018 - acc: 0.9719\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1018 - acc: 0.9718\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1017 - acc: 0.9717\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.1017 - acc: 0.9719\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.1017 - acc: 0.9717\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1016 - acc: 0.9719\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.1016 - acc: 0.9719\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1017 - acc: 0.9718\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1014 - acc: 0.9720\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.1013 - acc: 0.9719\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.1012 - acc: 0.9719\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.1013 - acc: 0.9719\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.1013 - acc: 0.9719\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.1012 - acc: 0.9720\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.1012 - acc: 0.9720\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 5s 159ms/step - loss: 0.1010 - acc: 0.9720\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.1010 - acc: 0.9720\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1010 - acc: 0.9720\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.1010 - acc: 0.9721\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.1009 - acc: 0.9721\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.1008 - acc: 0.9720\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.1008 - acc: 0.9721\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1008 - acc: 0.9719\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1006 - acc: 0.9721\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.1006 - acc: 0.9721\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1008 - acc: 0.9720\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.1006 - acc: 0.9721\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.1005 - acc: 0.9721\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1004 - acc: 0.9722\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1005 - acc: 0.9723\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 5s 155ms/step - loss: 0.1007 - acc: 0.9721\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.1004 - acc: 0.9722\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1001 - acc: 0.9724\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.1001 - acc: 0.9724\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1001 - acc: 0.9723\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1002 - acc: 0.9722\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 5s 150ms/step - loss: 0.1001 - acc: 0.9721\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.1001 - acc: 0.9723\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.1001 - acc: 0.9723\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 5s 146ms/step - loss: 0.1001 - acc: 0.9723\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.1001 - acc: 0.9724\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.1000 - acc: 0.9725\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 4s 141ms/step - loss: 0.0997 - acc: 0.9725\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 4s 132ms/step - loss: 0.0996 - acc: 0.9726\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0997 - acc: 0.9724\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 4s 136ms/step - loss: 0.0998 - acc: 0.9723\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 4s 136ms/step - loss: 0.0998 - acc: 0.9723\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0997 - acc: 0.9725\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 4s 134ms/step - loss: 0.0995 - acc: 0.9726\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 5s 144ms/step - loss: 0.0994 - acc: 0.9726\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0993 - acc: 0.9727\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 4s 132ms/step - loss: 0.0994 - acc: 0.9725\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 5s 142ms/step - loss: 0.0994 - acc: 0.9726\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0994 - acc: 0.9726\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 4s 132ms/step - loss: 0.0995 - acc: 0.9726\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 5s 145ms/step - loss: 0.0996 - acc: 0.9725\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0995 - acc: 0.9725\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.0994 - acc: 0.9724\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 5s 148ms/step - loss: 0.0994 - acc: 0.9724\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0991 - acc: 0.9726\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0991 - acc: 0.9727\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 5s 153ms/step - loss: 0.0990 - acc: 0.9726\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0989 - acc: 0.9727\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0989 - acc: 0.9727\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0989 - acc: 0.9726\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0990 - acc: 0.9727\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0991 - acc: 0.9726\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0993 - acc: 0.9725\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0991 - acc: 0.9726\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0990 - acc: 0.9727\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0989 - acc: 0.9726\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0989 - acc: 0.9725\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0989 - acc: 0.9726\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0989 - acc: 0.9727\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0989 - acc: 0.9725\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0988 - acc: 0.9725\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0986 - acc: 0.9727\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0985 - acc: 0.9727\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0988 - acc: 0.9726\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0987 - acc: 0.9726\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0989 - acc: 0.9725\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0988 - acc: 0.9727\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 5s 159ms/step - loss: 0.0984 - acc: 0.9728\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0985 - acc: 0.9726\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0986 - acc: 0.9727\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 5s 159ms/step - loss: 0.0984 - acc: 0.9728\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0983 - acc: 0.9728\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0981 - acc: 0.9728\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0983 - acc: 0.9728\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0983 - acc: 0.9728\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0981 - acc: 0.9730\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0979 - acc: 0.9730\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0981 - acc: 0.9729\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0979 - acc: 0.9730\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0980 - acc: 0.9730\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0980 - acc: 0.9729\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0978 - acc: 0.9731\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0978 - acc: 0.9730\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0978 - acc: 0.9730\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0978 - acc: 0.9731\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0979 - acc: 0.9729\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0981 - acc: 0.9729\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0980 - acc: 0.9728\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0977 - acc: 0.9731\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0976 - acc: 0.9731\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0975 - acc: 0.9731\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 5s 153ms/step - loss: 0.0975 - acc: 0.9730\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0976 - acc: 0.9731\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0975 - acc: 0.9731\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 5s 150ms/step - loss: 0.0977 - acc: 0.9730\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.0979 - acc: 0.9729\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0975 - acc: 0.9729\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 5s 144ms/step - loss: 0.0974 - acc: 0.9732\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0974 - acc: 0.9732\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0976 - acc: 0.9730\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 4s 135ms/step - loss: 0.0977 - acc: 0.9729\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 4s 134ms/step - loss: 0.0975 - acc: 0.9730\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0973 - acc: 0.9732\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0974 - acc: 0.9730\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 5s 141ms/step - loss: 0.0973 - acc: 0.9731\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0971 - acc: 0.9732\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 4s 135ms/step - loss: 0.0969 - acc: 0.9733\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 5s 146ms/step - loss: 0.0971 - acc: 0.9731\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0971 - acc: 0.9732\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0971 - acc: 0.9733\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 5s 147ms/step - loss: 0.0972 - acc: 0.9732\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0971 - acc: 0.9733\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0972 - acc: 0.9732\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 5s 151ms/step - loss: 0.0969 - acc: 0.9734\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0972 - acc: 0.9731\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0975 - acc: 0.9729\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0972 - acc: 0.9731\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0971 - acc: 0.9732\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0967 - acc: 0.9733\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0966 - acc: 0.9733\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0966 - acc: 0.9733\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0966 - acc: 0.9733\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0967 - acc: 0.9733\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0970 - acc: 0.9732\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0969 - acc: 0.9732\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 5s 159ms/step - loss: 0.0969 - acc: 0.9732\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0968 - acc: 0.9732\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0967 - acc: 0.9733\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 5s 155ms/step - loss: 0.0965 - acc: 0.9734\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0966 - acc: 0.9733\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0964 - acc: 0.9735\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 0.0963 - acc: 0.9734\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0964 - acc: 0.9735\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0962 - acc: 0.9735\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0961 - acc: 0.9736\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0961 - acc: 0.9736\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0964 - acc: 0.9735\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0966 - acc: 0.9734\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0966 - acc: 0.9731\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0967 - acc: 0.9733\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 5s 159ms/step - loss: 0.0964 - acc: 0.9735\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0962 - acc: 0.9736\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0966 - acc: 0.9735\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0962 - acc: 0.9735\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0964 - acc: 0.9734\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0964 - acc: 0.9734\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 5s 160ms/step - loss: 0.0964 - acc: 0.9734\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0962 - acc: 0.9734\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0964 - acc: 0.9733\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0963 - acc: 0.9734\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0959 - acc: 0.9735\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0960 - acc: 0.9735\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0960 - acc: 0.9736\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0959 - acc: 0.9735\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0961 - acc: 0.9736\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0959 - acc: 0.9736\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0959 - acc: 0.9736\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0958 - acc: 0.9736\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 5s 161ms/step - loss: 0.0958 - acc: 0.9736\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0959 - acc: 0.9736\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0957 - acc: 0.9737\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0956 - acc: 0.9736\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0955 - acc: 0.9737\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0957 - acc: 0.9736\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0956 - acc: 0.9736\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0954 - acc: 0.9737\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0955 - acc: 0.9736\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 0.0956 - acc: 0.9737\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0956 - acc: 0.9737\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0954 - acc: 0.9737\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 5s 157ms/step - loss: 0.0953 - acc: 0.9739\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0952 - acc: 0.9738\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe36e0322f0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
        "\n",
        "def decode(x):\n",
        "  if x == 0: return \" \"\n",
        "  return reverse_map[x]"
      ],
      "metadata": {
        "id": "FF2ufF0JmyKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_soft_for_symbol(model, size, sentence, temperature=1.0):\n",
        "   start = sentence\n",
        "   for i in range(0, size):\n",
        "     ch = tokenizer.texts_to_sequences(start)[0]\n",
        "     out1 = tf.one_hot(ch, vocab_size)\n",
        "     out1 = tf.reshape(out1, shape=(1, out1.shape[0], out1.shape[1]))\n",
        "     out1 = tf.keras.preprocessing.sequence.pad_sequences(out1,padding='post',maxlen=maxlen)\n",
        "     logits = model(out1)[0][-1]\n",
        "     probs = tf.exp(tf.math.log(logits) / temperature).numpy().astype(np.float64)\n",
        "     probs = probs / np.sum(probs)\n",
        "     nc = np.argmax(np.random.multinomial(1, probs, 1))\n",
        "     print(decode(nc), end='')\n",
        "     if nc == eos_token: break\n",
        "     start = decode(nc)"
      ],
      "metadata": {
        "id": "X8cBs7qYg42b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"empty\", \"boring\", \"happy\", \"bridge\", \"you\", \"where\", \"when\", \"take\", \"us\", \"want\", \"remember\", \"friendship\", \"coming\", \"expected\", \"make\", \"do\", \"all\", \"birthday\", \"chance\"]\n",
        "temperature_rates = [1, 1.2, 1.4, 1.6]\n",
        "for word in start_words:\n",
        "  for temperature in temperature_rates:\n",
        "    print(\"[START WORD]: \", word, \" [TEMPERATURE]:\", temperature)\n",
        "    generate_soft_for_symbol(symbol_simple_rnn, 20, [word], temperature=temperature)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "DEqCfXADiBa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16931c71-2a7e-4285-dd4a-6b3bb291e4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START WORD]:  empty  [TEMPERATURE]: 1\n",
            "inonouzzzzzzzzzzzzzz\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.2\n",
            " Whencono.<eos>\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.4\n",
            "palouzzzzzzzzzzzzzza\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.6\n",
            "w!<eos>\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1\n",
            "inon'sered ANonere's\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.2\n",
            "oinonelere Whedous S\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.4\n",
            "everevererell\"<eos>\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.6\n",
            "erononelllsed ecal's\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1\n",
            " Wouthanowh, Tanexav\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.2\n",
            " Covenoredinekizzzzz\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.4\n",
            " Dofol.<eos>\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.6\n",
            "!<eos>\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1\n",
            "se!<eos>\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.2\n",
            "s Bun'sered Wh, WO-(\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.4\n",
            "llounofreremevereven\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.6\n",
            "s WOfivepavedy methy\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1\n",
            " Siveviveverevedonon\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.2\n",
            ", Werelineliverevedo\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.4\n",
            "'ved Anostheredamone\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.6\n",
            ". hevenal'melowleve-\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1\n",
            " Buzzzzzzzzzzzzzzzzz\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.2\n",
            " Thedononthevedevelc\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.4\n",
            " medinonodada thello\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.6\n",
            " teded'mel'mo-Hevex!\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1\n",
            "eve.<eos>\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.2\n",
            "ell've A: Yome CHa A\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.4\n",
            "eeveeveven, Wh-(Jure\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.6\n",
            "ef Buzzzzzzzzzzzzzzz\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1\n",
            " tollliveleveve?<eos>\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.2\n",
            " Donousthachin'veder\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.4\n",
            " quth!<eos>\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.6\n",
            "! Cash!<eos>\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1\n",
            ". Buthexedave.<eos>\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.2\n",
            "cus.<eos>\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.4\n",
            "ed Uh, RUhinedomela \n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.6\n",
            ", Wh, Cot, Wey! Yoll\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1\n",
            "?<eos>\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.2\n",
            ".<eos>\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.4\n",
            " omouled'sere.<eos>\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.6\n",
            "ext l'medomeredahera\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1\n",
            "izzzzzzzzzzzalllonov\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.2\n",
            "?<eos>\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.4\n",
            ", Sow!<eos>\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.6\n",
            " Pledysen's! frexR.<eos>\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1\n",
            " Alonout'sedonof (MA\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.2\n",
            " h!<eos>\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.4\n",
            ".<eos>\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.6\n",
            ".. Spe. llonorelidin\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1\n",
            " Wevededousexeredere\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.2\n",
            "!<eos>\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.4\n",
            "!<eos>\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.6\n",
            ", Douzzzzzerexikavot\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1\n",
            " Wedevevelanch, Cofo\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.2\n",
            "n'velinomouzzzzzzzzz\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.4\n",
            " Yomal'vesededomello\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.6\n",
            "s! Wh-Qud Hanflrevev\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1\n",
            "dononodomedolonovedo\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.2\n",
            ", CHevererelederered\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.4\n",
            ", Thanopexi-(Canolyo\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.6\n",
            "thedilll'mou'topoved\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1\n",
            "'medovered!<eos>\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.2\n",
            "no! Yollomy. pouzzzz\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.4\n",
            "whexccButhers, Okan'\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.6\n",
            "'s!<eos>\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1\n",
            "inoulithavelanevedy \n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.2\n",
            " Whemedohidelllidono\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.4\n",
            "'st. Uhesemellon'ved\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.6\n",
            "! medowrinellinoffre\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1\n",
            " Gous!<eos>\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.2\n",
            " Wononoulo!<eos>\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.4\n",
            "? Bodoveredidowacthe\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.6\n",
            "?<eos>\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1\n",
            "!<eos>\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.2\n",
            "dedon's, wome Jul'me\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.4\n",
            "dodomede. An'veredy \n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.6\n",
            "dof Sllomp? foredove\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Однослойная LSTM"
      ],
      "metadata": {
        "id": "eEEffiNiiJ8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_lstm = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Masking(input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size,activation='softmax'))\n",
        "])\n",
        "symbol_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFKYNygemt0b",
        "outputId": "1e4094b2-4606-47a2-d60f-948d30725c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_1 (Masking)         (None, 312, 83)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 312, 128)          108544    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 312, 83)          10707     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 119,251\n",
            "Trainable params: 119,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "symbol_lstm.fit(x1, y1, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IQCyBEWXmuE3",
        "outputId": "cb222438-8d99-4576-fbcf-8f18ab0475e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 16s 387ms/step - loss: 0.9441 - acc: 0.8540\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 13s 390ms/step - loss: 0.5833 - acc: 0.8817\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.7296 - acc: 0.8521\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.4149 - acc: 0.9000\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.3921 - acc: 0.9012\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.3720 - acc: 0.9035\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.3498 - acc: 0.9085\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.3280 - acc: 0.9132\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.3115 - acc: 0.9161\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 13s 392ms/step - loss: 0.2989 - acc: 0.9176\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2888 - acc: 0.9196\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.2803 - acc: 0.9216\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2732 - acc: 0.9233\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2664 - acc: 0.9249\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.2606 - acc: 0.9263\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2554 - acc: 0.9278\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.2508 - acc: 0.9289\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.2463 - acc: 0.9299\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2416 - acc: 0.9314\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 12s 392ms/step - loss: 0.2377 - acc: 0.9321\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.2341 - acc: 0.9332\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.2307 - acc: 0.9340\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.2265 - acc: 0.9351\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.2228 - acc: 0.9360\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 13s 392ms/step - loss: 0.2195 - acc: 0.9369\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2158 - acc: 0.9379\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 12s 391ms/step - loss: 0.2128 - acc: 0.9385\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.2099 - acc: 0.9393\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 12s 392ms/step - loss: 0.2071 - acc: 0.9399\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.2042 - acc: 0.9407\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.2014 - acc: 0.9411\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1986 - acc: 0.9419\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1963 - acc: 0.9425\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.1931 - acc: 0.9434\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1904 - acc: 0.9440\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1873 - acc: 0.9449\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 12s 391ms/step - loss: 0.1844 - acc: 0.9457\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 12s 390ms/step - loss: 0.1823 - acc: 0.9463\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 12s 386ms/step - loss: 0.1797 - acc: 0.9468\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 12s 379ms/step - loss: 0.1776 - acc: 0.9476\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 13s 392ms/step - loss: 0.1742 - acc: 0.9485\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 13s 384ms/step - loss: 0.1722 - acc: 0.9489\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 12s 381ms/step - loss: 0.1693 - acc: 0.9498\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 12s 374ms/step - loss: 0.1671 - acc: 0.9503\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 12s 379ms/step - loss: 0.1645 - acc: 0.9513\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 13s 388ms/step - loss: 0.1621 - acc: 0.9517\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.1599 - acc: 0.9526\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1572 - acc: 0.9534\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.1541 - acc: 0.9543\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.1520 - acc: 0.9547\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 13s 392ms/step - loss: 0.1493 - acc: 0.9553\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 13s 391ms/step - loss: 0.1465 - acc: 0.9563\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1439 - acc: 0.9569\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.1416 - acc: 0.9575\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.1393 - acc: 0.9583\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.1363 - acc: 0.9591\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.1340 - acc: 0.9598\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1320 - acc: 0.9604\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.1301 - acc: 0.9611\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1267 - acc: 0.9618\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1230 - acc: 0.9629\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 13s 392ms/step - loss: 0.1212 - acc: 0.9635\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1188 - acc: 0.9641\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 12s 391ms/step - loss: 0.1166 - acc: 0.9644\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1133 - acc: 0.9656\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1110 - acc: 0.9663\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.1095 - acc: 0.9666\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 12s 392ms/step - loss: 0.1064 - acc: 0.9678\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.1062 - acc: 0.9677\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1036 - acc: 0.9682\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.1022 - acc: 0.9687\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.1006 - acc: 0.9690\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 12s 392ms/step - loss: 0.0984 - acc: 0.9699\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0960 - acc: 0.9705\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0931 - acc: 0.9715\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0917 - acc: 0.9719\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0899 - acc: 0.9724\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0897 - acc: 0.9723\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0854 - acc: 0.9737\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0850 - acc: 0.9738\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0843 - acc: 0.9740\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0835 - acc: 0.9739\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0800 - acc: 0.9753\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0781 - acc: 0.9758\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0750 - acc: 0.9768\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0751 - acc: 0.9766\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 12s 391ms/step - loss: 0.0738 - acc: 0.9771\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 12s 385ms/step - loss: 0.0749 - acc: 0.9766\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 12s 375ms/step - loss: 0.0746 - acc: 0.9766\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 13s 388ms/step - loss: 0.0711 - acc: 0.9778\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 12s 379ms/step - loss: 0.0687 - acc: 0.9787\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 13s 390ms/step - loss: 0.0683 - acc: 0.9786\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 12s 382ms/step - loss: 0.0685 - acc: 0.9786\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 13s 391ms/step - loss: 0.0664 - acc: 0.9793\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 13s 391ms/step - loss: 0.0660 - acc: 0.9793\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.0663 - acc: 0.9791\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 13s 385ms/step - loss: 0.0647 - acc: 0.9795\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.0619 - acc: 0.9805\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.0615 - acc: 0.9807\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 13s 391ms/step - loss: 0.0610 - acc: 0.9809\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.0590 - acc: 0.9816\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0582 - acc: 0.9818\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0562 - acc: 0.9824\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0570 - acc: 0.9820\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0564 - acc: 0.9824\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0551 - acc: 0.9826\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0550 - acc: 0.9826\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0577 - acc: 0.9816\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0585 - acc: 0.9810\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 12s 391ms/step - loss: 0.0591 - acc: 0.9808\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0546 - acc: 0.9826\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0534 - acc: 0.9831\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0532 - acc: 0.9829\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 13s 404ms/step - loss: 0.0522 - acc: 0.9832\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0511 - acc: 0.9839\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0517 - acc: 0.9833\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0499 - acc: 0.9842\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0477 - acc: 0.9850\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0481 - acc: 0.9846\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 13s 402ms/step - loss: 0.0478 - acc: 0.9846\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0468 - acc: 0.9853\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0463 - acc: 0.9854\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0461 - acc: 0.9852\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0493 - acc: 0.9842\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0501 - acc: 0.9836\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0465 - acc: 0.9851\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0455 - acc: 0.9854\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0495 - acc: 0.9837\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0496 - acc: 0.9838\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0482 - acc: 0.9844\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0460 - acc: 0.9850\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0455 - acc: 0.9852\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0542 - acc: 0.9820\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0519 - acc: 0.9826\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0483 - acc: 0.9841\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0444 - acc: 0.9858\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0402 - acc: 0.9873\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0403 - acc: 0.9873\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 13s 406ms/step - loss: 0.0383 - acc: 0.9880\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0386 - acc: 0.9878\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0401 - acc: 0.9871\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0388 - acc: 0.9877\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.0372 - acc: 0.9882\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0361 - acc: 0.9888\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0360 - acc: 0.9887\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 13s 393ms/step - loss: 0.0365 - acc: 0.9886\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0388 - acc: 0.9875\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0389 - acc: 0.9874\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0390 - acc: 0.9874\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.0444 - acc: 0.9851\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0457 - acc: 0.9848\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0459 - acc: 0.9846\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.0455 - acc: 0.9847\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0439 - acc: 0.9853\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0416 - acc: 0.9862\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 13s 396ms/step - loss: 0.0411 - acc: 0.9863\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 13s 390ms/step - loss: 0.0390 - acc: 0.9872\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 13s 384ms/step - loss: 0.0380 - acc: 0.9877\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 13s 389ms/step - loss: 0.0374 - acc: 0.9880\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 12s 381ms/step - loss: 0.0352 - acc: 0.9887\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 12s 382ms/step - loss: 0.0357 - acc: 0.9885\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 13s 390ms/step - loss: 0.0347 - acc: 0.9889\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 12s 378ms/step - loss: 0.0346 - acc: 0.9889\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 13s 387ms/step - loss: 0.0390 - acc: 0.9872\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 13s 392ms/step - loss: 0.0409 - acc: 0.9863\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0410 - acc: 0.9863\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0399 - acc: 0.9867\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0399 - acc: 0.9869\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0380 - acc: 0.9875\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0352 - acc: 0.9886\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 13s 395ms/step - loss: 0.0335 - acc: 0.9890\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 13s 394ms/step - loss: 0.0351 - acc: 0.9887\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 13s 402ms/step - loss: 0.0353 - acc: 0.9884\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.0355 - acc: 0.9885\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0338 - acc: 0.9890\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0323 - acc: 0.9896\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0343 - acc: 0.9888\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 13s 397ms/step - loss: 0.0359 - acc: 0.9881\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0401 - acc: 0.9864\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.0410 - acc: 0.9861\n",
            "Epoch 181/200\n",
            "10/32 [========>.....................] - ETA: 10s - loss: 0.0396 - acc: 0.9864"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c4755d586d59>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msymbol_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msymbol_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"empty\", \"boring\", \"happy\", \"bridge\"  \"you\", \"where\", \"when\", \"take\", \"us\", \"want\", \"remember\", \"friendship\", \"coming\", \"expected\", \"make\", \"do\", \"all\", \"birthday\", \"chance\"]\n",
        "temperature_rates = [1, 1.2, 1.4, 1.6]\n",
        "for word in start_words:\n",
        "  for temperature in temperature_rates:\n",
        "    print(\"[START WORD]: \", word, \" [TEMPERATURE]:\", temperature)\n",
        "    generate_soft_for_symbol(symbol_lstm, 20, [word], temperature=temperature)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWj2QlcwmuT0",
        "outputId": "f6fa1fcb-e28f-445e-c53f-d2f300d1e94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START WORD]:  empty  [TEMPERATURE]: 1\n",
            "........... HAluthet\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.2\n",
            "..........MOfrevesth\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.4\n",
            ". l Yousthemecadyonv\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.6\n",
            "......balthinvanvest\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1\n",
            "....................\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.2\n",
            "sthevadyonvestheJure\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.4\n",
            "sthecanedofreveuth-T\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.6\n",
            "sth, Onvesthecanvany\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1\n",
            "............ So lf B\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.2\n",
            " A Nofrecasthesthanv\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.4\n",
            " Buzek Ahanvem6.. Co\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.6\n",
            "? lthexquzerecaltThe\n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1\n",
            "s Sonveconveth-(Bume\n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1.2\n",
            "pencal'mestrevetstha\n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1.4\n",
            "d julfifo AltwistAve\n",
            "\n",
            "[START WORD]:  bridgeyou  [TEMPERATURE]: 1.6\n",
            "zYeThanvacadreth-ANo\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1\n",
            " nvexthenvethecanexc\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.2\n",
            " win'vecanve9'vecado\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.4\n",
            " Necanvethecanveveth\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.6\n",
            " SofonvezistAwoutr(I\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1\n",
            " Althecasthecanvenve\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.2\n",
            " Juthetlth-(KI Nored\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.4\n",
            " wethe8Youzenvestha2\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.6\n",
            " Theltheas-AUhevetAl\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1\n",
            "sthanvedlfonvethethe\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.2\n",
            "... l Binethenvedore\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.4\n",
            "mumecofoucancanvavex\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.6\n",
            ".. Thameksto Lesanve\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1\n",
            "th, Wofofof ltheco W\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.2\n",
            "eretonves Ofonveveca\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.4\n",
            "evecancanvext Lecano\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.6\n",
            " Tonvetrecath, uthhe\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1\n",
            "ifrethalsthenvecanve\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.2\n",
            "ifristhecanveve7-ATu\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.4\n",
            "ifiveved l jutrethal\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.6\n",
            "ifinthe(HutwadoDonve\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1\n",
            " TOhadadgonveth-Buze\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.2\n",
            " Wen'xcat Yetheco Se\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.4\n",
            " BYe'medanvet-Co Bul\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.6\n",
            "t5HeveHetASmet Jusve\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1\n",
            " Anvevesthanvenvethe\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.2\n",
            " Trethevecousth-Okpe\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.4\n",
            "..................Fa\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.6\n",
            " Seuth, TRHEtheveth-\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1\n",
            " Slfouth-(Shevelth, \n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.2\n",
            "! Sof Awenvethecalfo\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.4\n",
            "senvestAThet l Secav\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.6\n",
            " Buxth-Oknvecalfonve\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1\n",
            " Nonvethestwif BUhec\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.2\n",
            " wifinifrethan'me& g\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.4\n",
            " ANoretA Owin-KI'Lof\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.6\n",
            "..........snvelfous-\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1\n",
            "nenvethecanvecAnveth\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.2\n",
            "evecanvevethextheze-\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.4\n",
            ", Binvevecadifofo MA\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.6\n",
            ", YestnnveZLecayo, w\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1\n",
            "onvecanvevethecofonv\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.2\n",
            "onvenvenveth........\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.4\n",
            "onverenofroforecasta\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.6\n",
            "o re'vedo Leth...3 J\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1\n",
            " Yonetheveth-Youthec\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.2\n",
            "? Nofresthethedonvec\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.4\n",
            " Yet BOofo BOh-TYofr\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.6\n",
            "nethayourecasthefron\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1\n",
            " Alfo k.............\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.2\n",
            " l'Ponexthetheclsth \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.4\n",
            " Alth-(Sofo Thonvare\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.6\n",
            " HextA MyofreMAereth\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1\n",
            "donvethanvetha......\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.2\n",
            "dinveva Youzecadisth\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.4\n",
            " eedYofrevevestheth,\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.6\n",
            "donecanvecab BUhetlt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Многослойная LSTM"
      ],
      "metadata": {
        "id": "IqR4Bf8JobaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_multilayer_lstm = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Masking(input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size,activation='softmax'))\n",
        "])\n",
        "symbol_multilayer_lstm.summary()"
      ],
      "metadata": {
        "id": "wJLpUpBuoeWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4da43d8-567c-4290-9255-2b86e293ee54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_2 (Masking)         (None, 312, 83)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 312, 128)          108544    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 312, 128)          131584    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 312, 128)          131584    \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 312, 83)          10707     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 382,419\n",
            "Trainable params: 382,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_multilayer_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "symbol_multilayer_lstm.fit(x1, y1, epochs=200)"
      ],
      "metadata": {
        "id": "Zytcm6fKojrz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "ba4ec7db-e265-40d7-fc2d-843d7df83ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "17/32 [==============>...............] - ETA: 20s - loss: 0.0254 - acc: 0.9921"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-82b704743418>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msymbol_multilayer_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msymbol_multilayer_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"empty\", \"boring\", \"happy\", \"bridge\",  \"you\", \"where\", \"when\", \"take\", \"us\", \"want\", \"remember\", \"friendship\", \"coming\", \"expected\", \"make\", \"do\", \"all\", \"birthday\", \"chance\"]\n",
        "temperature_rates = [1, 1.2, 1.4, 1.6]\n",
        "for word in start_words:\n",
        "  for temperature in temperature_rates:\n",
        "    print(\"[START WORD]: \", word, \" [TEMPERATURE]:\", temperature)\n",
        "    generate_soft_for_symbol(symbol_multilayer_lstm, 20, [word], temperature=temperature)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "70ph6P6_oj1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fdbadf-5836-4899-bf0d-7d36023db495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START WORD]:  empty  [TEMPERATURE]: 1\n",
            " Ex; Altweveva Whano\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.2\n",
            " B-Hevanowivevevl WO\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.4\n",
            " Nevlfreveva Tutw We\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.6\n",
            "  Arevotwanofrevre W\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1\n",
            "! WOha WOhanofoltwis\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.2\n",
            " Cofofofofofofol'LOh\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.4\n",
            " Whe-Jufok2.WOhevano\n",
            "\n",
            "[START WORD]:  boring  [TEMPERATURE]: 1.6\n",
            " Yofofofivevanofoutw\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1\n",
            "nofrevevevevltweveve\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.2\n",
            " Whaltutwlfofofetwiv\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.4\n",
            "ivev SLofowivavltw C\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.6\n",
            "lfotanoltwiveveStwif\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1\n",
            "danofrevltwive'vivev\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.2\n",
            "da Nofreveveveva ltw\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.4\n",
            "s Yofreva Anofoca St\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.6\n",
            "nl Ta Any.pevanovevi\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1\n",
            "revevevevreva Al Wha\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.2\n",
            "div Alfofofrevanofre\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.4\n",
            "'NofofrevafreveveRof\n",
            "\n",
            "[START WORD]:  you  [TEMPERATURE]: 1.6\n",
            "'meveve ltwitwav l W\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1\n",
            " Yocanofofrevreva fr\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.2\n",
            " Lofofofofreva Neva \n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.4\n",
            "lltweve'fofrofreva W\n",
            "\n",
            "[START WORD]:  where  [TEMPERATURE]: 1.6\n",
            "anofreva AhaltofAreR\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1\n",
            ",evanofofofofofrevev\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.2\n",
            ",evlta Whanofrevltwe\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.4\n",
            "twivevevrevevevevavl\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.6\n",
            ",eva-B-Qutwanofofwev\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1\n",
            ",evanhanofofrevevl'v\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.2\n",
            ",evevevevevevanofokg\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.4\n",
            " Bufranofofre Ohev T\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.6\n",
            ",a CofreutwevanofoxH\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1\n",
            "eva Owistweva Wha Co\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.2\n",
            "evevanofolfotweva No\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.4\n",
            "ev BOhaveveva Leveva\n",
            "\n",
            "[START WORD]:  us  [TEMPERATURE]: 1.6\n",
            "evevevegufreve Lltwi\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1\n",
            ",eveveva Wevevanowiv\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.2\n",
            ",evanofrevowevAltwan\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.4\n",
            ",evevavevanofre'veve\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.6\n",
            ",anoltwiveva Whaniva\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1\n",
            "twevevevanofofreveve\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.2\n",
            "evIt Nok l Myutweviv\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.4\n",
            ",eve' Tiveve-Hanofof\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.6\n",
            "spocanofonowigrevl'N\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1\n",
            ",evevreva Whavevevev\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.2\n",
            "! l,e"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fe36ddb9f90>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5117, in <genexpr>\n",
            "    output_ta_t = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 243, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "veveve'v Yevlfo\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.4\n",
            ".....wivava Ogevevev\n",
            "\n",
            "[START WORD]:  friendship  [TEMPERATURE]: 1.6\n",
            ",eOfopivivev WOhanof\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1\n",
            " Tha Ohanofofohanofo\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.2\n",
            " YofreveveveOkbofrev\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.4\n",
            " CHofofofofrevlt lta\n",
            "\n",
            "[START WORD]:  coming  [TEMPERATURE]: 1.6\n",
            " BOnlfofreveve'stwiv\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1\n",
            " Cofofotwiveva Wheve\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.2\n",
            ",evanofre'vevlfrevan\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.4\n",
            " Arevanofofo-xPofofr\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.6\n",
            " Oh- Al YofotOha Wha\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1\n",
            ",eva Al Arevevevevev\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.2\n",
            ",eva Jutwa Ooforeva \n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.4\n",
            ",evevetwival'vltweve\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.6\n",
            ",et Wha Nevevlwre're\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1\n",
            "utwiveveveRevltwever\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.2\n",
            "utwevrevevev CHAl Ju\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.4\n",
            "ufrevl YOha ANeva Wh\n",
            "\n",
            "[START WORD]:  do  [TEMPERATURE]: 1.6\n",
            "utwevrevl BltwaOfofo\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1\n",
            ",eva Altwevevltwivan\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.2\n",
            "...............- Jut\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.4\n",
            " Alfofofreva CHeveve\n",
            "\n",
            "[START WORD]:  all  [TEMPERATURE]: 1.6\n",
            ",eva ltwevlfreOwilfo\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1\n",
            " AltwivrevevevExzlfo\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.2\n",
            " Yevltwiltwaltwevan \n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.4\n",
            " Yevevevivanha Coofo\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.6\n",
            " A l Yel'v Cofoofofr\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1\n",
            ",evevevl Tha CHofrev\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.2\n",
            ",eveveveva Lofreveve\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.4\n",
            "da l'vlfrevevevivano\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.6\n",
            "! TutwivevltRe Lot A\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Двунаправленная LSTM"
      ],
      "metadata": {
        "id": "pEZR17IMvTX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bidirectional_lstm = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Masking(input_shape=(maxlen,vocab_size)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size))),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size))),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True,input_shape=(maxlen,vocab_size))),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size,activation='softmax'))\n",
        "])\n",
        "bidirectional_lstm.summary()"
      ],
      "metadata": {
        "id": "rtCCpcu2vXn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fb59c0-8ad7-43ef-a4eb-e2a701fb0d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_4 (Masking)         (None, 312, 83)           0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 312, 256)         217088    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 312, 256)         394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 312, 256)         394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, 312, 83)          21331     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,026,899\n",
            "Trainable params: 1,026,899\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bidirectional_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "bidirectional_lstm.fit(x1, y1, epochs=200)"
      ],
      "metadata": {
        "id": "26pVJFJ0vXve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "07f3e5c0-d484-4607-8497-ab10cb893b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 118s 3s/step - loss: 0.9291 - acc: 0.8593\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.4257 - acc: 0.9006\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.3953 - acc: 0.9024\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 93s 3s/step - loss: 0.3778 - acc: 0.9040\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.3627 - acc: 0.9055\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.3522 - acc: 0.9066\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.3427 - acc: 0.9071\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 93s 3s/step - loss: 0.3298 - acc: 0.9087\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.3105 - acc: 0.9119\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 93s 3s/step - loss: 0.2796 - acc: 0.9206\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 93s 3s/step - loss: 0.2277 - acc: 0.9362\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 94s 3s/step - loss: 0.1696 - acc: 0.9532\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 93s 3s/step - loss: 0.1159 - acc: 0.9715\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 93s 3s/step - loss: 0.0786 - acc: 0.9833\n",
            "Epoch 15/200\n",
            "15/32 [=============>................] - ETA: 51s - loss: 0.0585 - acc: 0.9891"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-edd47d5dc193>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbidirectional_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbidirectional_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"empty\", \"happy\", \"bridge\", \"when\", \"take\", \"want\", \"remember\", \"expected\", \"make\", \"birthday\", \"chance\"]\n",
        "temperature_rates = [1, 1.2, 1.4, 1.6]\n",
        "for word in start_words:\n",
        "  for temperature in temperature_rates:\n",
        "    print(\"[START WORD]: \", word, \" [TEMPERATURE]:\", temperature)\n",
        "    generate_soft_for_symbol(symbol_multilayer_lstm, 20, [word], temperature=temperature)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "GEJmtB6hvX7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2bd16e-19c9-487e-b3a8-36fe3a57bf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START WORD]:  empty  [TEMPERATURE]: 1\n",
            " Whanofrevevanofreva\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.2\n",
            " Tha Alfofrevivevano\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.4\n",
            " lfre'vevevavevere'v\n",
            "\n",
            "[START WORD]:  empty  [TEMPERATURE]: 1.6\n",
            " TOw Whenofrevevev C\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1\n",
            " Nevanofofrevevevano\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.2\n",
            " Butwa Nofrevetwevev\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.4\n",
            "'v TevltwiveveDolfre\n",
            "\n",
            "[START WORD]:  happy  [TEMPERATURE]: 1.6\n",
            "iveMoxpofreDReve:an \n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1\n",
            "danotwivanyutwivevav\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.2\n",
            "ltwivev Weva ROfofre\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.4\n",
            "danonofrevlt BUhanof\n",
            "\n",
            "[START WORD]:  bridge  [TEMPERATURE]: 1.6\n",
            "notwevevltofrevanowe\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1\n",
            "twivanofofrevevanofr\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.2\n",
            " Avev Wha Nofreveva \n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.4\n",
            ",egevltwivifretwivev\n",
            "\n",
            "[START WORD]:  when  [TEMPERATURE]: 1.6\n",
            "hevevofreveOnofofo-(\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1\n",
            ",evavevevevevevanofr\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.2\n",
            ",evevevevevanofrevan\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.4\n",
            ",evltofofretwivltwiv\n",
            "\n",
            "[START WORD]:  take  [TEMPERATURE]: 1.6\n",
            ",evevLiveva JufreveR\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1\n",
            ",evExGofofrevavevive\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.2\n",
            "ivevevevevanofreva T\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.4\n",
            " Nofmanofrevanogevev\n",
            "\n",
            "[START WORD]:  want  [TEMPERATURE]: 1.6\n",
            ",evevanivevevalt Ono\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1\n",
            ",evevanofreva Beveve\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.2\n",
            ",evanofofretwevanolt\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.4\n",
            "twivanofreveveve'veR\n",
            "\n",
            "[START WORD]:  remember  [TEMPERATURE]: 1.6\n",
            ",revofofrevreva'vege\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1\n",
            " WOwevevevanofofreve\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.2\n",
            " Yowevevlt Tofreveva\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.4\n",
            " A'ltwevevevl'frev W\n",
            "\n",
            "[START WORD]:  expected  [TEMPERATURE]: 1.6\n",
            ".......nof Nlilt Mfo\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1\n",
            ",eva Yofofrevevivevi\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.2\n",
            ",evev Whavevevevavan\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.4\n",
            " Yan BUheveRofreYeve\n",
            "\n",
            "[START WORD]:  make  [TEMPERATURE]: 1.6\n",
            " TOnowivltutwevzltwe\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1\n",
            " Wevevlfrevetwivevev\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.2\n",
            " Thanowivevanofrevev\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.4\n",
            " Stwi4_ofoeveva Shan\n",
            "\n",
            "[START WORD]:  birthday  [TEMPERATURE]: 1.6\n",
            " Wevavofofrevanok5iv\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1\n",
            " Wheveveveva Butwiva\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.2\n",
            ",eveve&re'vanofrevof\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.4\n",
            ",e'Nev-Okpetwevano M\n",
            "\n",
            "[START WORD]:  chance  [TEMPERATURE]: 1.6\n",
            " Wha Alfohhanocofofr\n",
            "\n"
          ]
        }
      ]
    }
  ]
}