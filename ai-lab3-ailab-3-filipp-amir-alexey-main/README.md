[![Open in Visual Studio Code](https://classroom.github.com/assets/open-in-vscode-718a45dd9cf7e7f842a935f5ebbe5719a5e09af4491e668f4dbf3b35d5cca122.svg)](https://classroom.github.com/online_ide?assignment_repo_id=10881163&assignment_repo_type=AssignmentRepo)
# Лабораторная работа по курсу "Искусственный интеллект"
# Создание своего нейросетевого фреймворка

### Студенты: 

| ФИО       | Роль в проекте                     | Оценка       |
|-----------|------------------------------------|--------------|
| Хашимов Амир Азизович | Руководил и применял нейросеть в классических задачах |          |
| Абдуль-Хади Филипп Ибрагим | Программировал создание сети и функционал работы с данными и писал отчет |       |
| Зайцев Кирилл Владимирович | Программировал оптимизаторы и писал отчет |      |
|  Шар Алексей Михайлович  | Программировал обучение нейронной сети и писал отчет |          |

> *Комментарии проверяющего*

### Задание

Реализовать свой нейросетевой фреймворк для обучения полносвязных нейросетей, который должен включать в себя следующие возможности:

Документация к Фреймворку
1. Введение
<br> Реализован фреймворк для работы с многослойными нейросетями на C# (реализовано обучение, вспомогательные функции). Есть поддержка нескольких разных функций активаций.
2. Оптимизация
  - SGD (Stochastic Gradient Descent) - это один из наиболее распространенных алгоритмов оптимизации для обучения нейронных сетей. Он заключается в обновлении весов модели на основе градиента функции потерь для каждого примера из тренировочного набора. Это означает, что веса модели обновляются после каждой итерации, что делает SGD быстрым и эффективным в использовании для больших наборов данных. Однако, из-за случайного выбора примеров в каждой итерации, SGD может быть шумным и медленным в сходимости.

  - Adam (Adaptive Moment Estimation) - это алгоритм оптимизации, который использует движение среднего и несмещенную оценку второго момента градиента для адаптивного выбора скорости обучения. Он позволяет быстрее сходиться к оптимальной точке, а также обеспечивает устойчивость при изменении градиента. Adam также является одним из наиболее распространенных алгоритмов оптимизации для обучения нейронных сетей.

  - Nesterov (Nesterov Accelerated Gradient) - это усовершенствованный вариант метода градиентного спуска. В Nesterov Momentum используется "momentum" (импульс) для ускорения сходимости, при этом "импульс" определяет в каком направлении нужно двигаться, основываясь на предыдущих шагах. Это позволяет более быстро сходиться к оптимальной точке. Nesterov Momentum также является устойчивым к изменению градиента.

  - В целом, выбор определенного алгоритма оптимизации зависит от конкретной задачи и типа данных, на которых вы обучаете модель. SGD хорошо работает для больших наборов данных, но может быть медленным в сходимости на некоторых задачах. Adam показывает хорошие результаты на большинстве задач и является одним из наиболее популярных алгоритмов оптимизации. Nesterov также является эффективным алгоритмом оптимизации, но не так часто используется в сравнении с SGD и Adam.

3. Функции потерь
<br> MSE (Mean Squared Error) и Cross Entropy Loss являются двумя широко используемыми функциями потерь в глубоком обучении. Они оба используются для измерения ошибки прогнозирования модели, но для разных типов задач.

- MSE (Mean Squared Error) - это функция потерь, которая используется в задачах регрессии. Она вычисляет среднее значение квадратов разностей между прогнозируемыми и реальными значениями. 

- Cross Entropy Loss - это функция потерь, которая широко используется в задачах классификации. Она измеряет ошибку между прогнозируемыми вероятностями классов и реальными метками классов.

4. Передаточные функции
<br> Softmax, Hyperbolic Tangent (Tanh) и Rectified Linear Unit (ReLU) - это передаточные функции, которые широко используются в глубоком обучении.

- Softmax - это функция активации, которая используется для преобразования выходных значений в вероятности классов в задачах классификации. 
- Hyperbolic Tangent (Tanh) - это функция активации, которая часто используется в скрытых слоях нейронных сетей. Она преобразует входное значение в диапазоне от -1 до 1
- Rectified Linear Unit (ReLU) - это функция активации, которая используется в скрытых слоях нейронных сетей. Она применяет линейную активацию, если входное значение положительное, и нулевую активацию, если входное значение отрицательное.

<br> Каждая из этих функций активации имеет свои преимущества и недостатки в зависимости от типа задачи и структуры нейронной сети. Например, Softmax хорошо работает в задачах классификации с многими классами, Tanh может помочь в уменьшении градиентного исчезновения, а ReLU может ускорить сходимость обучения в некоторых случаях.

5. Примеры использования 
<br> Для тестирования нашей нейросети на основе базы данных MNIST, которая содержит в себе набор рукописных цифр, мы использовали следующие параметры нейросети

- Функция активации (передаточная функция) - Softmax. Данный выбор был обусловлен тем, что нам необходимо нормализовать вероятности по выбору среди 10 различных независимых вариантов - цифры от 0 до 9. Как было сказано ранее, данная функция активации позволяет ограничить наши выходные данные таким образом, что сумма вероятностей на выходном слое даст нам в итоге 1. 
- Функция потерь - Cross Entropy Loss. В отличии от классической среднеквадратичной функции потерь, данная функция позволяет оценить ошибку в тех случаях, когда мы решаем задачу классификации, и наши данные на выходном слое являются независимыми. Данную особенность функция MSE не несет.
- Оптимизатор - SGD. За данным выбором не стоит большой логики, просто данный оптимизатор обладает самой высокой скоростью выполнения на некоторой итерации.  
 
6. Вывод
<br> На сегодняшний день многие нейросетевые фреймворки имеют много реализаций на языке python. В ходе данной лабораторной работы нам предоставилась возмоность написать свой аналог популярных фреймворков на другом не менее используемом языке программирования c#. Во время работы мы тщательно изучили внутренню анатомию нейросети, ознакомилимь со всей работой с матрицами и, что является важнейшим в нашей работе, мы освоили метод обратного распространения ошибки, который и сотворяет всю магию компьютерного интеллекта. 
<br> Нам довелось реализовать несколько функций минимизации ошибки, а также несколько вариаций самих таких функций. Кроме того наша архитектура позволяет создать и обучить нейросеть в несколько строк, что на сегодняшний день является важнейшей частью многих нейросетевых фреймворков.


